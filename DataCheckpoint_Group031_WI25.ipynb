{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you lost points on the last checkpoint you can get them back by responding to TA/IA feedback**  \n",
    "\n",
    "Update/change the relevant sections where you lost those points, make sure you respond on GitHub Issues to your TA/IA to call their attention to the changes you made here.\n",
    "\n",
    "Please update your Timeline... no battle plan survives contact with the enemy, so make sure we understand how your plans have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Ant Man\n",
    "- Hulk\n",
    "- Iron Man\n",
    "- Thor\n",
    "- Wasp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Include a specific, clear data science question.\n",
    "-  Make sure what you're measuring (variables) to answer the question is clear\n",
    "\n",
    "What is your research question? Include the specific question you're setting out to answer. This question should be specific, answerable with data, and clear. A general question with specific subquestions is permitted. (1-2 sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Include a general introduction to your topic                       <---- I dont think we did this well enough on the proposal.\n",
    "- Include explanation of what work has been done previously\n",
    "- Include citations or links to previous work\n",
    "\n",
    "This section will present the background and context of your topic and question in a few paragraphs. Include a general introduction to your topic and then describe what information you currently know about the topic after doing your initial research. Include references to other projects who have asked similar questions or approached similar problems. Explain what others have learned in their projects.\n",
    "\n",
    "Find some relevant prior work, and reference those sources, summarizing what each did and what they learned. Even if you think you have a totally novel question, find the most similar prior work that you can and discuss how it relates to your project.\n",
    "\n",
    "References can be research publications, but they need not be. Blogs, GitHub repositories, company websites, etc., are all viable references if they are relevant to your project. It must be clear which information comes from which references. (2-3 paragraphs, including at least 2 references)\n",
    "\n",
    " **Use inline citation through HTML footnotes to specify which references support which statements** \n",
    "\n",
    "From the Amazon Product Pricing Report 2024 on Issuu<a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1), we can see that Amazon prices are influenced by a vast amount of factors such as supply and demand, seasonal trends, competition, shifting seller fees, algorithmic pricing, Amazon's buy box system. Other elements such as brand power, customer reviews, and holiday shopping behavior also contribute to pricing variability. This report serves as essential information for this project as it provides context to better interpret and analyze Amazon price trends. The report provides information on different pricing trends across different sectors of the market such as beauty, home and kitchen, arts and crafts, pet supply, and baby products, and demonstrates how each sector faces different trends. With each market sector, the report also produces concise and clear visualizations of pricing data across multiple Amazon products.\n",
    "\n",
    "The Consumer Price Index (CPI) Summary from the US Bureau of Labor Statistics<a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) offers guidance as to how to structure and analyze data related to our topic. Their report on CPI changes from 2023-2024 exemplifies how to organize large datasets and distill them into clear, actionable insights. The summary’s consistent formatting and emphasis on year-over-year percentage changes allow for a straightforward understanding of trends in consumer prices across different sectors. The structured approach will be instrumental in our own analysis of pricing data, helping us standardize our methodology and avoid potential misinterpretations. By adopting their organizational strategy, we can enhance the accuracy and credibility of our findings.\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Jungle Scout (2024, Jan). Amazon Product Pricing Report 2024. Issuu. https://issuu.com/junglescoutcobalt/docs/jungle-scout-amazon-product-pricing-report-2024?utm_source=chatgpt.html \n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Bureau of Labor Statistics (2024, Feb). Consumer Price Index Summary. U.S. Department of Labor. https://www.bls.gov/news.release/pdf/cpi.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Include your team's hypothesis\n",
    "- Ensure that this hypothesis is clear to readers\n",
    "- Explain why you think this will be the outcome (what was your thinking?)\n",
    "\n",
    "What is your main hypothesis/predictions about what the answer to your question is? Briefly explain your thinking. (2-3 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "- etc\n",
    "\n",
    "Now write 2 - 5 sentences describing each dataset here. Include a short description of the important variables in the dataset; what the metrics and datatypes are, what concepts they may be proxies for. Include information about how you would need to wrangle/clean/preprocess the dataset\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the keepa library (run this cell if not already installed)\n",
    "#!pip install keepa\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import keepa\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"df2mtauj1tmrngcm95ubshd41fplpf2bfh1nba8s8hpd2m6golbbrj9bat7osb8o\" # do no share outside of private repo!!\n",
    "api = keepa.Keepa(ACCESS_KEY, timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the price of a given amazon product, there are many different types of the the 'price' variable we can access. Two of them are new price and listing price. Here are the differences:\n",
    "\n",
    "• NEW PRICE:\n",
    "This is the current selling price for an item offered on Amazon in brand new condition. It reflects the actual market price that customers pay—typically the lowest available offer among Amazon and third‑party sellers. Because it’s influenced by promotions, competition, and real‑time market conditions, the NEW price can fluctuate over time.\n",
    "\n",
    "• LISTING PRICE:\n",
    "This is usually the manufacturer’s suggested retail price (MSRP) or the original price displayed on the product’s listing. It tends to be more stable and is often used as a reference to show discounts or price reductions. Even when the new price drops (for deals or competitive reasons), the listing price may remain unchanged.\n",
    "\n",
    "The new price shows you what you’d pay right now, while the listing price is a reference value set by the manufacturer. This difference helps sellers and buyers gauge the discount depth and market dynamics.\n",
    "\n",
    "Since we want to accurately capture the price a consumer is paying at a given time, we will use the NEW PRICE.\n",
    "\n",
    "Other price variables we can access are the NEW_FBM (Filled by Manufacturer), NEW_FBA (Filled by Amazon), USED, (used items), REFURBISHED (refurbished items), WAREHOUSE (prices from amazon warhouse deals, usually returned items). These price variables offer spotty data coverage, and don't accurately represent the consumer experience, so we will not be using them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Price History Dataset from Keepa API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to gather the ASINs (Amazon Standard Identification Number) for each product category. Each product on Amazon has a unique ASIN. We can collect them manually through the Keepa Data Product Finder tool. Here we filter by category: baby products, and rank: 1-1000. This gathers the top 1000 product ASINs for baby products. Additionally we include the name of the product, and the sub category. We made sure to exclude 'variations' of products, because we dont want to sample the same product 5 times just because there are 5 versions of the product with slight variations. We also refined our query to only physical products, excluding digital products and ebooks. CPI data is only collected on physical goods, so this allows us to have a fair comparison. And our final product filter is 'tracking time' which allows us to only select products that actually have price data from 2021-2024. If we tracked items that were listed in 2023, we would only have 1 year of price data for that item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in top 1000 baby products csv:\n",
    "data_path = Path('data') / 'baby_products.csv'\n",
    "products = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pampers Baby Diapers - Swaddlers - Size 8, 60 ...</td>\n",
       "      <td>disposable diapers</td>\n",
       "      <td>B0C88BKX13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WaterWipes Plastic-Free Original 99.9% Water B...</td>\n",
       "      <td>wipes &amp; holders</td>\n",
       "      <td>B0C7LW9C7H</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Huggies Natural Care Sensitive Baby Wipes, Uns...</td>\n",
       "      <td>wipes &amp; refills</td>\n",
       "      <td>B08QRT84WJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pampers Baby Wipes Sensitive, Water Based Wipe...</td>\n",
       "      <td>wipes &amp; refills</td>\n",
       "      <td>B0BJ14MYC9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Honest Company Clean Conscious Unscented W...</td>\n",
       "      <td>wipes &amp; refills</td>\n",
       "      <td>B0DCHPP188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Huggies Size 1 Diapers, Little Snugglers Newbo...</td>\n",
       "      <td>disposable diapers</td>\n",
       "      <td>B09NS37ZHX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>No-Touch Thermometer for Adults and Kids, Digi...</td>\n",
       "      <td>thermometers</td>\n",
       "      <td>B0CC29Y419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Huggies Size 4 Diapers, Little Movers Baby Dia...</td>\n",
       "      <td>disposable diapers</td>\n",
       "      <td>B08VC33TDN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dr. Brown’s Natural Flow Level 2 &amp; Level 3 Nar...</td>\n",
       "      <td>nipples</td>\n",
       "      <td>B09VCTN7M6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Huggies Simply Clean Unscented Baby Diaper Wip...</td>\n",
       "      <td>wipes &amp; refills</td>\n",
       "      <td>B08QRKY3NJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       product_title         subcategory  \\\n",
       "0  Pampers Baby Diapers - Swaddlers - Size 8, 60 ...  disposable diapers   \n",
       "1  WaterWipes Plastic-Free Original 99.9% Water B...     wipes & holders   \n",
       "2  Huggies Natural Care Sensitive Baby Wipes, Uns...     wipes & refills   \n",
       "3  Pampers Baby Wipes Sensitive, Water Based Wipe...     wipes & refills   \n",
       "4  The Honest Company Clean Conscious Unscented W...     wipes & refills   \n",
       "5  Huggies Size 1 Diapers, Little Snugglers Newbo...  disposable diapers   \n",
       "6  No-Touch Thermometer for Adults and Kids, Digi...        thermometers   \n",
       "7  Huggies Size 4 Diapers, Little Movers Baby Dia...  disposable diapers   \n",
       "8  Dr. Brown’s Natural Flow Level 2 & Level 3 Nar...             nipples   \n",
       "9  Huggies Simply Clean Unscented Baby Diaper Wip...     wipes & refills   \n",
       "\n",
       "         asin  \n",
       "0  B0C88BKX13  \n",
       "1  B0C7LW9C7H  \n",
       "2  B08QRT84WJ  \n",
       "3  B0BJ14MYC9  \n",
       "4  B0DCHPP188  \n",
       "5  B09NS37ZHX  \n",
       "6  B0CC29Y419  \n",
       "7  B08VC33TDN  \n",
       "8  B09VCTN7M6  \n",
       "9  B08QRKY3NJ  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ASINs, product name, and subcategory, we've noticed that some products have multiple sub categories, many of which are irrelevant for the purposes of our analysis such as \"Baby Coupons\", \"TEST ABCDEFGPD\" and even random characters such as \"d963aedb-8e7e-493c...\". We want to remove clean the subcategory column so that each product has a single subcategory. Luckily for us, it seems that the first sub category is the most descriptive subcategory of a given product, so we can remove the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subcategory\n",
       "Home & Kitchen, Letters & Numbers, Baby                                                                                          1\n",
       "Medicine Dispensers, Fresh | Household Coupons, Fresh | HPC Coupons, Fresh | Health & Household Coupons, Fresh | Baby Coupons    1\n",
       "Safety, Safety                                                                                                                   1\n",
       "Meals, d963aedb-8e7e-493c-80e7-b70e3c253824_9301, Gluten-Free Groceries, Baby Food                                               1\n",
       "Tandem                                                                                                                           1\n",
       "Toy Chests & Organizers, Categories                                                                                              1\n",
       "Convertible Cribs, Kitchen & Dining Features, Nursery                                                                            1\n",
       "Hair Care                                                                                                                        1\n",
       "Smart Monitors, Safety, Inspirations from your Baby Registry Welcome Box                                                         1\n",
       "Stroller Hooks                                                                                                                   1\n",
       "Snack Foods, Plant-Based Lifestyles, Grocery: More Sustainable Products, Organic Groceries                                       1\n",
       "Meals, TEST ABCDEFGPD, Baby Food Subscribe & Save, Baby Food                                                                     1\n",
       "Storage & Organizers                                                                                                             1\n",
       "Scales                                                                                                                           1\n",
       "Bottle-Feeding, Solid Feeding, Feeding Tools & Accessories                                                                       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['subcategory'].value_counts().tail(15) # bottom 15 subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function remove extra subcategories from each row and rename columns\n",
    "def clean_frame(df):\n",
    "    df = df.rename(columns = {'Title' : 'product_title', 'Categories: Sub' : 'subcategory', 'ASIN' : 'asin'})\n",
    "    def clean_row(row):\n",
    "        row['subcategory'] = row['subcategory'].split(',')[0].strip().lower()\n",
    "        return row\n",
    "    df = df.apply(clean_row, axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "products = clean_frame(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subcategory\n",
       "convertible cribs              1\n",
       "tandem                         1\n",
       "gate extensions                1\n",
       "strap & belt covers            1\n",
       "harnesses & leashes            1\n",
       "travel carry bags              1\n",
       "pillow covers                  1\n",
       "glider & ottoman sets          1\n",
       "furniture                      1\n",
       "prenatal monitoring devices    1\n",
       "bottle handles                 1\n",
       "lamps & shades                 1\n",
       "stroller hooks                 1\n",
       "storage & organizers           1\n",
       "scales                         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['subcategory'].value_counts().tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['subcategory'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our least common subcategories are actually meaningful, and we are left with 179 subcategories for baby products.\n",
    "\n",
    "Moving forward, we can use the ASINs column to query the Keepa API for historical price data of each item. With our basic membership access to the Keepa API, we are limited with how much data we can request, so we will have to produce our price data in batches. Within the function get_monthly_avg_prices, we query specific ASINs to get their historical price data, and return a dataframe of this data. Due to the API limitations, we have to incrementally build up a large csv of price data for different product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepa_time_to_datetime(kt):\n",
    "    # Convert Keepa time (minutes since 2011-01-01) to a Python datetime (UTC)\n",
    "    # According to Keepa docs:  Unix timestamp = (keepaTime + 21564000) * 60\n",
    "    # If kt is already a datetime, just return it\n",
    "    if isinstance(kt, datetime.datetime):\n",
    "        return kt\n",
    "    # Otherwise, assume it's a Keepa integer time\n",
    "    return datetime.datetime.utcfromtimestamp((kt + 21564000) * 60)\n",
    "\n",
    "def get_monthly_avg_prices(asins, days=1460):\n",
    "    \"\"\"\n",
    "    asins: list of ASIN strings\n",
    "    days: how many days of history to request (default 1460 ~ 4 years)\n",
    "    \n",
    "    Returns a DataFrame:\n",
    "        - Rows = ASINs\n",
    "        - Columns = monthly time periods (e.g. '2023-01', '2023-02', etc.)\n",
    "        - Values = average 'NEW' price for that month\n",
    "    \"\"\"\n",
    "    # Single API call for all ASINs\n",
    "    products = api.query(asins, days=days)\n",
    "\n",
    "    dfs = []\n",
    "    for product in products:\n",
    "        asin = product['asin']\n",
    "        \n",
    "        # Extract price/time arrays for the 'NEW' price\n",
    "        price_history = product['data'].get('LISTPRICE', [])\n",
    "        time_history  = product['data'].get('LISTPRICE_time', [])\n",
    "        \n",
    "        if len(price_history) == 0 or len(time_history) == 0:\n",
    "            # If no price data, skip or create an empty frame for this ASIN\n",
    "            continue\n",
    "        \n",
    "        # Convert Keepa time to datetime, convert price from cents to dollars\n",
    "        dates = [keepa_time_to_datetime(t) for t in time_history]\n",
    "        prices = [p for p in price_history]\n",
    "        \n",
    "        # Create a DF for this ASIN\n",
    "        df = pd.DataFrame({'date': dates, asin: prices})\n",
    "        df.set_index('date', inplace=True)\n",
    "        \n",
    "        # Resample to monthly average (end-of-month). \n",
    "        # If you prefer start-of-month, use 'MS' or '30D' as needed.\n",
    "        monthly_avg = df.resample('M').mean()\n",
    "        \n",
    "        # monthly_avg now has a DateTimeIndex, and 1 column named after the ASIN.\n",
    "        dfs.append(monthly_avg)\n",
    "    \n",
    "    if not dfs:\n",
    "        return pd.DataFrame()  # No data, return empty\n",
    "    \n",
    "    # Concatenate on axis=1 => date index, one column per ASIN\n",
    "    combined = pd.concat(dfs, axis=1)\n",
    "    \n",
    "    # Transpose so rows = ASIN, columns = monthly date\n",
    "    combined = combined.T\n",
    "    \n",
    "    # Optionally convert the DatetimeIndex to strings like 'YYYY-MM'\n",
    "    combined.columns = [col.strftime('%Y-%m') for col in combined.columns]\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def batch(iterable, n=25):\n",
    "    \"\"\"Yield successive n-sized chunks from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:08<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE, cost 1 token:\n",
    "asins = [\"B0C7FBH4QV\",'B0DJ1Z3XW4','B0CZ4KLN8Q']  \n",
    "\n",
    "test = api.query(asins, days = 365)\n",
    "\n",
    "#monthly_prices = get_monthly_avg_prices(asins)\n",
    "#monthly_prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.datetime(2023, 6, 8, 20, 8),\n",
       "       datetime.datetime(2023, 6, 12, 3, 52),\n",
       "       datetime.datetime(2023, 6, 14, 1, 4),\n",
       "       datetime.datetime(2023, 6, 15, 7, 18),\n",
       "       datetime.datetime(2023, 6, 20, 14, 44),\n",
       "       datetime.datetime(2023, 6, 23, 6, 4),\n",
       "       datetime.datetime(2023, 6, 26, 13, 40),\n",
       "       datetime.datetime(2023, 6, 27, 21, 46),\n",
       "       datetime.datetime(2023, 6, 28, 20, 56),\n",
       "       datetime.datetime(2023, 6, 29, 4, 18),\n",
       "       datetime.datetime(2023, 7, 1, 21, 28),\n",
       "       datetime.datetime(2023, 7, 2, 13, 8),\n",
       "       datetime.datetime(2023, 7, 3, 21, 8),\n",
       "       datetime.datetime(2023, 7, 4, 5, 12),\n",
       "       datetime.datetime(2023, 7, 4, 6, 48),\n",
       "       datetime.datetime(2023, 7, 20, 16, 32),\n",
       "       datetime.datetime(2023, 7, 21, 13, 40),\n",
       "       datetime.datetime(2023, 7, 30, 22, 16),\n",
       "       datetime.datetime(2023, 7, 31, 21, 22),\n",
       "       datetime.datetime(2023, 8, 5, 10, 0),\n",
       "       datetime.datetime(2023, 8, 5, 13, 32),\n",
       "       datetime.datetime(2023, 8, 7, 16, 12),\n",
       "       datetime.datetime(2023, 8, 7, 19, 14),\n",
       "       datetime.datetime(2023, 8, 10, 3, 36),\n",
       "       datetime.datetime(2023, 8, 10, 10, 32),\n",
       "       datetime.datetime(2023, 8, 13, 20, 24),\n",
       "       datetime.datetime(2023, 8, 14, 16, 48),\n",
       "       datetime.datetime(2023, 8, 15, 15, 40),\n",
       "       datetime.datetime(2023, 8, 15, 16, 40),\n",
       "       datetime.datetime(2023, 8, 15, 19, 4),\n",
       "       datetime.datetime(2023, 8, 16, 17, 24),\n",
       "       datetime.datetime(2023, 8, 16, 19, 20),\n",
       "       datetime.datetime(2023, 8, 16, 21, 38),\n",
       "       datetime.datetime(2023, 8, 17, 19, 4),\n",
       "       datetime.datetime(2023, 8, 18, 22, 16),\n",
       "       datetime.datetime(2023, 8, 18, 23, 50),\n",
       "       datetime.datetime(2023, 8, 19, 1, 58),\n",
       "       datetime.datetime(2023, 8, 19, 19, 36),\n",
       "       datetime.datetime(2023, 8, 20, 20, 18),\n",
       "       datetime.datetime(2023, 8, 21, 1, 52),\n",
       "       datetime.datetime(2023, 8, 23, 0, 46),\n",
       "       datetime.datetime(2023, 8, 23, 23, 8),\n",
       "       datetime.datetime(2023, 8, 24, 0, 36),\n",
       "       datetime.datetime(2023, 8, 24, 3, 4),\n",
       "       datetime.datetime(2023, 8, 24, 4, 4),\n",
       "       datetime.datetime(2023, 8, 24, 10, 44),\n",
       "       datetime.datetime(2023, 8, 25, 1, 14),\n",
       "       datetime.datetime(2023, 8, 25, 4, 2),\n",
       "       datetime.datetime(2023, 8, 26, 0, 34),\n",
       "       datetime.datetime(2023, 8, 26, 0, 52),\n",
       "       datetime.datetime(2023, 8, 26, 3, 44),\n",
       "       datetime.datetime(2023, 8, 26, 19, 36),\n",
       "       datetime.datetime(2023, 8, 27, 9, 10),\n",
       "       datetime.datetime(2023, 8, 28, 20, 4),\n",
       "       datetime.datetime(2023, 8, 28, 23, 44),\n",
       "       datetime.datetime(2023, 8, 29, 2, 4),\n",
       "       datetime.datetime(2023, 8, 29, 5, 52),\n",
       "       datetime.datetime(2023, 8, 29, 18, 16),\n",
       "       datetime.datetime(2023, 8, 29, 20, 14),\n",
       "       datetime.datetime(2023, 8, 30, 0, 20),\n",
       "       datetime.datetime(2023, 8, 30, 18, 46),\n",
       "       datetime.datetime(2023, 8, 30, 20, 26),\n",
       "       datetime.datetime(2023, 8, 30, 21, 56),\n",
       "       datetime.datetime(2023, 8, 31, 4, 52),\n",
       "       datetime.datetime(2023, 8, 31, 8, 36),\n",
       "       datetime.datetime(2023, 8, 31, 11, 36),\n",
       "       datetime.datetime(2023, 8, 31, 12, 12),\n",
       "       datetime.datetime(2023, 8, 31, 14, 42),\n",
       "       datetime.datetime(2023, 8, 31, 20, 4),\n",
       "       datetime.datetime(2023, 8, 31, 20, 46),\n",
       "       datetime.datetime(2023, 8, 31, 21, 26),\n",
       "       datetime.datetime(2023, 8, 31, 22, 24),\n",
       "       datetime.datetime(2023, 8, 31, 22, 30),\n",
       "       datetime.datetime(2023, 9, 1, 1, 40),\n",
       "       datetime.datetime(2023, 9, 1, 3, 42),\n",
       "       datetime.datetime(2023, 9, 1, 4, 20),\n",
       "       datetime.datetime(2023, 9, 1, 7, 4),\n",
       "       datetime.datetime(2023, 9, 1, 11, 34),\n",
       "       datetime.datetime(2023, 9, 1, 15, 2),\n",
       "       datetime.datetime(2023, 9, 1, 17, 44),\n",
       "       datetime.datetime(2023, 9, 1, 18, 56),\n",
       "       datetime.datetime(2023, 9, 1, 21, 8),\n",
       "       datetime.datetime(2023, 9, 2, 0, 16),\n",
       "       datetime.datetime(2023, 9, 9, 0, 56),\n",
       "       datetime.datetime(2023, 9, 9, 20, 52),\n",
       "       datetime.datetime(2023, 9, 12, 16, 4),\n",
       "       datetime.datetime(2023, 9, 12, 19, 16),\n",
       "       datetime.datetime(2023, 9, 13, 21, 42),\n",
       "       datetime.datetime(2023, 9, 13, 22, 44),\n",
       "       datetime.datetime(2023, 9, 14, 1, 52),\n",
       "       datetime.datetime(2023, 9, 14, 20, 20),\n",
       "       datetime.datetime(2023, 9, 14, 22, 4),\n",
       "       datetime.datetime(2023, 9, 15, 17, 4),\n",
       "       datetime.datetime(2023, 9, 16, 8, 32),\n",
       "       datetime.datetime(2023, 9, 16, 14, 24),\n",
       "       datetime.datetime(2023, 9, 16, 18, 4),\n",
       "       datetime.datetime(2023, 9, 16, 23, 20),\n",
       "       datetime.datetime(2023, 9, 17, 5, 24),\n",
       "       datetime.datetime(2023, 9, 17, 7, 32),\n",
       "       datetime.datetime(2023, 9, 18, 17, 40),\n",
       "       datetime.datetime(2023, 9, 19, 3, 44),\n",
       "       datetime.datetime(2023, 9, 19, 16, 32),\n",
       "       datetime.datetime(2023, 9, 22, 10, 48),\n",
       "       datetime.datetime(2023, 9, 29, 22, 24),\n",
       "       datetime.datetime(2023, 10, 5, 21, 38),\n",
       "       datetime.datetime(2023, 10, 6, 3, 40),\n",
       "       datetime.datetime(2023, 10, 6, 9, 0),\n",
       "       datetime.datetime(2023, 10, 6, 12, 22),\n",
       "       datetime.datetime(2023, 10, 6, 21, 46),\n",
       "       datetime.datetime(2023, 10, 6, 22, 48),\n",
       "       datetime.datetime(2023, 10, 7, 3, 48),\n",
       "       datetime.datetime(2023, 10, 7, 4, 10),\n",
       "       datetime.datetime(2023, 10, 7, 9, 50),\n",
       "       datetime.datetime(2023, 10, 7, 12, 56),\n",
       "       datetime.datetime(2023, 10, 7, 21, 20),\n",
       "       datetime.datetime(2023, 10, 7, 21, 54),\n",
       "       datetime.datetime(2023, 10, 8, 10, 0),\n",
       "       datetime.datetime(2023, 10, 9, 7, 40),\n",
       "       datetime.datetime(2023, 10, 10, 1, 36),\n",
       "       datetime.datetime(2023, 10, 10, 3, 12),\n",
       "       datetime.datetime(2023, 10, 10, 4, 52),\n",
       "       datetime.datetime(2023, 10, 11, 0, 16),\n",
       "       datetime.datetime(2023, 10, 11, 3, 28),\n",
       "       datetime.datetime(2023, 10, 11, 23, 38),\n",
       "       datetime.datetime(2023, 10, 12, 3, 2),\n",
       "       datetime.datetime(2023, 10, 13, 17, 12),\n",
       "       datetime.datetime(2023, 10, 13, 22, 32),\n",
       "       datetime.datetime(2023, 10, 14, 19, 48),\n",
       "       datetime.datetime(2023, 10, 14, 23, 36),\n",
       "       datetime.datetime(2023, 10, 15, 3, 0),\n",
       "       datetime.datetime(2023, 10, 15, 6, 2),\n",
       "       datetime.datetime(2023, 10, 15, 8, 18),\n",
       "       datetime.datetime(2023, 10, 16, 21, 40),\n",
       "       datetime.datetime(2023, 10, 17, 0, 48),\n",
       "       datetime.datetime(2023, 10, 17, 5, 50),\n",
       "       datetime.datetime(2023, 10, 17, 10, 26),\n",
       "       datetime.datetime(2023, 10, 17, 22, 24),\n",
       "       datetime.datetime(2023, 10, 18, 3, 4),\n",
       "       datetime.datetime(2023, 10, 18, 22, 24),\n",
       "       datetime.datetime(2023, 10, 19, 4, 20),\n",
       "       datetime.datetime(2023, 10, 19, 19, 54),\n",
       "       datetime.datetime(2023, 10, 19, 21, 50),\n",
       "       datetime.datetime(2023, 10, 20, 0, 36),\n",
       "       datetime.datetime(2023, 10, 20, 11, 40),\n",
       "       datetime.datetime(2023, 10, 20, 14, 42),\n",
       "       datetime.datetime(2023, 10, 20, 19, 56),\n",
       "       datetime.datetime(2023, 10, 20, 23, 56),\n",
       "       datetime.datetime(2023, 10, 21, 18, 38),\n",
       "       datetime.datetime(2023, 10, 21, 19, 44),\n",
       "       datetime.datetime(2023, 10, 23, 19, 12),\n",
       "       datetime.datetime(2023, 10, 24, 3, 0),\n",
       "       datetime.datetime(2023, 10, 24, 18, 12),\n",
       "       datetime.datetime(2023, 10, 24, 19, 48),\n",
       "       datetime.datetime(2023, 10, 25, 1, 54),\n",
       "       datetime.datetime(2023, 10, 25, 2, 44),\n",
       "       datetime.datetime(2023, 10, 25, 5, 52),\n",
       "       datetime.datetime(2023, 10, 25, 11, 6),\n",
       "       datetime.datetime(2023, 10, 25, 20, 10),\n",
       "       datetime.datetime(2023, 10, 26, 2, 12),\n",
       "       datetime.datetime(2023, 10, 26, 4, 2),\n",
       "       datetime.datetime(2023, 10, 26, 12, 10),\n",
       "       datetime.datetime(2023, 10, 26, 18, 4),\n",
       "       datetime.datetime(2023, 10, 26, 20, 24),\n",
       "       datetime.datetime(2023, 10, 26, 20, 54),\n",
       "       datetime.datetime(2023, 10, 26, 23, 30),\n",
       "       datetime.datetime(2023, 10, 27, 17, 16),\n",
       "       datetime.datetime(2023, 10, 28, 0, 10),\n",
       "       datetime.datetime(2023, 10, 28, 20, 44),\n",
       "       datetime.datetime(2023, 10, 29, 2, 6),\n",
       "       datetime.datetime(2023, 10, 29, 5, 14),\n",
       "       datetime.datetime(2023, 10, 29, 16, 16),\n",
       "       datetime.datetime(2023, 10, 29, 20, 0),\n",
       "       datetime.datetime(2023, 10, 29, 20, 54),\n",
       "       datetime.datetime(2023, 10, 30, 1, 54),\n",
       "       datetime.datetime(2023, 10, 30, 2, 56),\n",
       "       datetime.datetime(2023, 10, 30, 3, 16),\n",
       "       datetime.datetime(2023, 10, 30, 18, 6),\n",
       "       datetime.datetime(2023, 10, 30, 18, 40),\n",
       "       datetime.datetime(2023, 10, 31, 1, 24),\n",
       "       datetime.datetime(2023, 10, 31, 9, 8),\n",
       "       datetime.datetime(2023, 10, 31, 16, 44),\n",
       "       datetime.datetime(2023, 10, 31, 20, 16),\n",
       "       datetime.datetime(2023, 11, 1, 13, 46),\n",
       "       datetime.datetime(2023, 11, 1, 21, 20),\n",
       "       datetime.datetime(2023, 11, 3, 7, 24),\n",
       "       datetime.datetime(2023, 11, 8, 14, 52),\n",
       "       datetime.datetime(2023, 11, 8, 22, 54),\n",
       "       datetime.datetime(2023, 11, 10, 5, 0),\n",
       "       datetime.datetime(2023, 11, 11, 8, 58),\n",
       "       datetime.datetime(2023, 11, 11, 15, 4),\n",
       "       datetime.datetime(2023, 11, 11, 23, 36),\n",
       "       datetime.datetime(2023, 11, 13, 15, 20),\n",
       "       datetime.datetime(2023, 11, 13, 20, 32),\n",
       "       datetime.datetime(2023, 11, 13, 23, 38),\n",
       "       datetime.datetime(2023, 11, 14, 2, 24),\n",
       "       datetime.datetime(2023, 11, 14, 6, 16),\n",
       "       datetime.datetime(2023, 11, 14, 12, 20),\n",
       "       datetime.datetime(2023, 11, 14, 17, 40),\n",
       "       datetime.datetime(2023, 11, 14, 23, 56),\n",
       "       datetime.datetime(2023, 11, 15, 0, 26),\n",
       "       datetime.datetime(2023, 11, 16, 18, 40),\n",
       "       datetime.datetime(2023, 11, 17, 0, 40),\n",
       "       datetime.datetime(2023, 11, 17, 3, 56),\n",
       "       datetime.datetime(2023, 11, 17, 10, 36),\n",
       "       datetime.datetime(2023, 11, 17, 20, 12),\n",
       "       datetime.datetime(2023, 11, 18, 0, 0),\n",
       "       datetime.datetime(2023, 11, 18, 23, 58),\n",
       "       datetime.datetime(2023, 11, 19, 0, 20),\n",
       "       datetime.datetime(2023, 11, 19, 21, 24),\n",
       "       datetime.datetime(2023, 11, 19, 23, 8),\n",
       "       datetime.datetime(2023, 11, 20, 16, 20),\n",
       "       datetime.datetime(2023, 11, 20, 19, 4),\n",
       "       datetime.datetime(2023, 11, 21, 19, 12),\n",
       "       datetime.datetime(2023, 11, 22, 23, 16),\n",
       "       datetime.datetime(2023, 11, 23, 6, 54),\n",
       "       datetime.datetime(2023, 11, 23, 10, 2),\n",
       "       datetime.datetime(2023, 11, 24, 1, 26),\n",
       "       datetime.datetime(2023, 11, 24, 19, 34),\n",
       "       datetime.datetime(2023, 11, 25, 21, 34),\n",
       "       datetime.datetime(2023, 11, 26, 20, 0),\n",
       "       datetime.datetime(2023, 11, 28, 12, 6),\n",
       "       datetime.datetime(2023, 11, 29, 5, 36),\n",
       "       datetime.datetime(2023, 11, 29, 14, 12),\n",
       "       datetime.datetime(2023, 11, 30, 8, 42),\n",
       "       datetime.datetime(2023, 11, 30, 20, 48),\n",
       "       datetime.datetime(2023, 12, 1, 5, 58),\n",
       "       datetime.datetime(2023, 12, 1, 17, 14),\n",
       "       datetime.datetime(2023, 12, 3, 3, 8),\n",
       "       datetime.datetime(2023, 12, 3, 9, 12),\n",
       "       datetime.datetime(2023, 12, 5, 8, 24),\n",
       "       datetime.datetime(2023, 12, 5, 22, 8),\n",
       "       datetime.datetime(2023, 12, 5, 23, 16),\n",
       "       datetime.datetime(2023, 12, 6, 1, 8),\n",
       "       datetime.datetime(2023, 12, 6, 2, 36),\n",
       "       datetime.datetime(2023, 12, 11, 16, 42),\n",
       "       datetime.datetime(2023, 12, 14, 21, 38),\n",
       "       datetime.datetime(2023, 12, 14, 22, 40),\n",
       "       datetime.datetime(2023, 12, 15, 2, 4),\n",
       "       datetime.datetime(2023, 12, 15, 10, 12),\n",
       "       datetime.datetime(2023, 12, 15, 10, 48),\n",
       "       datetime.datetime(2023, 12, 20, 17, 48),\n",
       "       datetime.datetime(2023, 12, 23, 0, 10),\n",
       "       datetime.datetime(2023, 12, 23, 3, 18),\n",
       "       datetime.datetime(2024, 1, 4, 18, 12),\n",
       "       datetime.datetime(2024, 1, 5, 18, 36),\n",
       "       datetime.datetime(2024, 1, 13, 17, 32),\n",
       "       datetime.datetime(2024, 1, 14, 10, 0),\n",
       "       datetime.datetime(2024, 1, 14, 16, 16),\n",
       "       datetime.datetime(2024, 1, 18, 1, 48),\n",
       "       datetime.datetime(2024, 1, 18, 5, 40),\n",
       "       datetime.datetime(2024, 1, 23, 3, 28),\n",
       "       datetime.datetime(2024, 1, 24, 3, 32),\n",
       "       datetime.datetime(2024, 2, 26, 4, 58),\n",
       "       datetime.datetime(2024, 2, 26, 22, 32),\n",
       "       datetime.datetime(2024, 2, 27, 1, 40),\n",
       "       datetime.datetime(2024, 2, 27, 8, 44),\n",
       "       datetime.datetime(2024, 2, 27, 19, 4),\n",
       "       datetime.datetime(2024, 2, 28, 1, 20),\n",
       "       datetime.datetime(2024, 2, 29, 3, 48),\n",
       "       datetime.datetime(2024, 2, 29, 7, 12),\n",
       "       datetime.datetime(2024, 2, 29, 16, 28),\n",
       "       datetime.datetime(2024, 2, 29, 21, 54),\n",
       "       datetime.datetime(2024, 3, 1, 3, 4),\n",
       "       datetime.datetime(2024, 3, 1, 7, 32),\n",
       "       datetime.datetime(2024, 3, 3, 2, 16),\n",
       "       datetime.datetime(2024, 3, 24, 15, 56),\n",
       "       datetime.datetime(2024, 3, 24, 20, 12),\n",
       "       datetime.datetime(2024, 3, 25, 0, 40),\n",
       "       datetime.datetime(2024, 3, 25, 3, 28),\n",
       "       datetime.datetime(2024, 3, 25, 6, 4),\n",
       "       datetime.datetime(2024, 3, 25, 17, 2),\n",
       "       datetime.datetime(2024, 3, 25, 21, 34),\n",
       "       datetime.datetime(2024, 3, 31, 2, 48),\n",
       "       datetime.datetime(2024, 4, 1, 5, 52),\n",
       "       datetime.datetime(2024, 4, 3, 2, 48),\n",
       "       datetime.datetime(2024, 4, 9, 23, 48),\n",
       "       datetime.datetime(2024, 4, 10, 4, 50),\n",
       "       datetime.datetime(2024, 4, 16, 0, 36),\n",
       "       datetime.datetime(2024, 4, 21, 15, 20),\n",
       "       datetime.datetime(2024, 4, 23, 1, 2),\n",
       "       datetime.datetime(2024, 4, 26, 22, 16),\n",
       "       datetime.datetime(2024, 4, 29, 12, 52),\n",
       "       datetime.datetime(2024, 4, 30, 1, 14),\n",
       "       datetime.datetime(2024, 5, 2, 1, 28),\n",
       "       datetime.datetime(2024, 5, 3, 15, 22),\n",
       "       datetime.datetime(2024, 5, 3, 18, 12),\n",
       "       datetime.datetime(2024, 5, 4, 18, 40),\n",
       "       datetime.datetime(2024, 5, 5, 5, 36),\n",
       "       datetime.datetime(2024, 5, 5, 22, 16),\n",
       "       datetime.datetime(2024, 5, 6, 4, 8),\n",
       "       datetime.datetime(2024, 5, 6, 9, 52),\n",
       "       datetime.datetime(2024, 5, 28, 3, 24),\n",
       "       datetime.datetime(2024, 5, 31, 20, 2),\n",
       "       datetime.datetime(2024, 6, 5, 0, 28),\n",
       "       datetime.datetime(2024, 6, 7, 4, 56),\n",
       "       datetime.datetime(2024, 6, 10, 2, 24),\n",
       "       datetime.datetime(2024, 6, 14, 0, 56),\n",
       "       datetime.datetime(2024, 6, 18, 0, 0),\n",
       "       datetime.datetime(2024, 6, 22, 18, 38),\n",
       "       datetime.datetime(2024, 6, 26, 23, 52),\n",
       "       datetime.datetime(2024, 7, 10, 1, 54),\n",
       "       datetime.datetime(2024, 7, 10, 5, 34),\n",
       "       datetime.datetime(2024, 7, 10, 6, 42),\n",
       "       datetime.datetime(2024, 7, 10, 8, 28),\n",
       "       datetime.datetime(2024, 7, 10, 17, 56),\n",
       "       datetime.datetime(2024, 7, 11, 13, 28),\n",
       "       datetime.datetime(2024, 7, 11, 16, 42),\n",
       "       datetime.datetime(2024, 7, 11, 20, 40),\n",
       "       datetime.datetime(2024, 7, 12, 2, 56),\n",
       "       datetime.datetime(2024, 7, 16, 7, 36),\n",
       "       datetime.datetime(2024, 7, 16, 14, 8),\n",
       "       datetime.datetime(2024, 7, 17, 10, 56),\n",
       "       datetime.datetime(2024, 7, 17, 15, 4),\n",
       "       datetime.datetime(2024, 7, 18, 7, 18),\n",
       "       datetime.datetime(2024, 7, 18, 17, 46),\n",
       "       datetime.datetime(2024, 7, 19, 2, 16),\n",
       "       datetime.datetime(2024, 7, 20, 19, 44),\n",
       "       datetime.datetime(2024, 7, 20, 20, 40),\n",
       "       datetime.datetime(2024, 7, 23, 12, 44),\n",
       "       datetime.datetime(2024, 7, 30, 18, 8),\n",
       "       datetime.datetime(2024, 7, 31, 1, 56),\n",
       "       datetime.datetime(2024, 8, 2, 2, 32),\n",
       "       datetime.datetime(2024, 8, 2, 11, 52),\n",
       "       datetime.datetime(2024, 8, 15, 13, 0),\n",
       "       datetime.datetime(2024, 8, 21, 1, 52),\n",
       "       datetime.datetime(2024, 8, 21, 4, 46),\n",
       "       datetime.datetime(2024, 8, 22, 12, 40),\n",
       "       datetime.datetime(2024, 9, 4, 9, 14),\n",
       "       datetime.datetime(2024, 9, 8, 15, 44),\n",
       "       datetime.datetime(2024, 9, 8, 16, 48),\n",
       "       datetime.datetime(2024, 9, 9, 2, 52),\n",
       "       datetime.datetime(2024, 10, 10, 4, 48),\n",
       "       datetime.datetime(2024, 10, 15, 11, 28),\n",
       "       datetime.datetime(2024, 10, 17, 3, 0),\n",
       "       datetime.datetime(2024, 10, 17, 9, 14),\n",
       "       datetime.datetime(2024, 10, 17, 17, 32),\n",
       "       datetime.datetime(2024, 10, 17, 22, 42),\n",
       "       datetime.datetime(2024, 11, 11, 5, 50),\n",
       "       datetime.datetime(2024, 11, 11, 8, 30),\n",
       "       datetime.datetime(2024, 11, 11, 23, 20),\n",
       "       datetime.datetime(2024, 11, 12, 11, 32),\n",
       "       datetime.datetime(2024, 11, 13, 2, 16),\n",
       "       datetime.datetime(2024, 11, 15, 16, 56),\n",
       "       datetime.datetime(2024, 11, 23, 15, 24),\n",
       "       datetime.datetime(2024, 12, 3, 18, 14),\n",
       "       datetime.datetime(2024, 12, 3, 22, 34),\n",
       "       datetime.datetime(2025, 1, 2, 6, 20),\n",
       "       datetime.datetime(2025, 1, 2, 9, 44),\n",
       "       datetime.datetime(2025, 1, 15, 4, 2),\n",
       "       datetime.datetime(2025, 1, 16, 20, 34),\n",
       "       datetime.datetime(2025, 1, 24, 17, 40),\n",
       "       datetime.datetime(2025, 1, 27, 16, 6),\n",
       "       datetime.datetime(2025, 2, 14, 5, 44),\n",
       "       datetime.datetime(2025, 2, 14, 6, 48),\n",
       "       datetime.datetime(2025, 2, 15, 6, 28),\n",
       "       datetime.datetime(2025, 2, 15, 20, 48),\n",
       "       datetime.datetime(2025, 2, 16, 10, 52),\n",
       "       datetime.datetime(2025, 2, 16, 19, 56),\n",
       "       datetime.datetime(2025, 2, 17, 7, 44),\n",
       "       datetime.datetime(2025, 2, 18, 0, 40),\n",
       "       datetime.datetime(2025, 2, 18, 12, 44),\n",
       "       datetime.datetime(2025, 2, 18, 18, 34)], dtype=object)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(test[0])\n",
    "test[0]['data'].get('NEW_time', [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the magic happens. With the function **query_keepa_in_batches**, we can incrementally build a csv of historical price data for a given category using the product dataframe we created earlier. The inputs to this function are: a dataframe such as baby_products, the category we are working with, i.e. 'baby_products', the number of batches we want, batch size, how many days of historical price data we want, and the start and stop indices in products dataframe. These start and stop indices allow us to pick up where we left off if for some reason we have to stop querying. We can just figure out the last batch we completed, multiply by 25, and keep going from there! If the CSV already exists, we just append to it. If it does not exist, we create a new one. This ensures we never overwrite data, and we can always keep track of where we are in the request process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual usage. Do not run unless necessary to conserve API tokens. CSV file is saved in data folder after each batch.\n",
    "# This may take several hours to run depending on max batches variable. Each ASIN costs 1 token, and current API subscription generates 1 token per minute. \n",
    "\n",
    "def query_keepa_in_batches(products, category, max_batches=10, batch_size=25, days=1460, start_index=0, stop_index=None):\n",
    "    \"\"\"\n",
    "    Query Keepa for monthly average prices in batches and incrementally save the results to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        products (DataFrame or dict): Contains product data with a key 'asin'.\n",
    "        category (str): Base name for the CSV file (e.g., 'baby_products').\n",
    "        max_batches (int): Number of batches to process (default 10). Each batch contains batch_size ASINs.\n",
    "        batch_size (int): Number of ASINs per batch (default 25).\n",
    "        days (int): Number of days of history to request (default 1460, ~4 years).\n",
    "        start_index (int): Start index for slicing the ASIN list.\n",
    "        stop_index (int or None): Stop index for slicing the ASIN list. If None, process until the end.\n",
    "        \n",
    "    Returns:\n",
    "        None. The function saves the data incrementally to a CSV file in the 'data' folder.\n",
    "    \"\"\"\n",
    "    # Slice the ASIN list based on start_index and stop_index.\n",
    "    asins = list(products['asin'])[start_index:stop_index]\n",
    "    \n",
    "    csv_file = f'data/{category}_monthly_prices.csv'\n",
    "    \n",
    "    for i, asin_batch in enumerate(batch(asins, batch_size)):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        df_batch = get_monthly_avg_prices(asin_batch, days=days)\n",
    "        # If the CSV file exists, append without headers; otherwise, create a new file with headers.\n",
    "        if os.path.exists(csv_file):\n",
    "            df_batch.to_csv(csv_file, mode='a', index=True, header=False)\n",
    "        else:\n",
    "            df_batch.to_csv(csv_file, index=True)\n",
    "        print(f\"Batch {i+1} processed and appended.\")\n",
    "\n",
    "\n",
    "# Example helper function to split the ASIN list into batches.\n",
    "def batch(iterable, n=25):\n",
    "    \"\"\"Yield successive n-sized chunks from iterable\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the boring part, waiting for the API requests as we slowly build up our csv. This code will take several hours to run as we have to accumulate tokens throughout the API request. It will probably run in the background and overnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_keepa_in_batches(products, 'baby_products', max_batches=10) # default batch size is 25, default days is 1460 (4 yrs),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entire process was just for the baby_products category. We will have to do this same process for every other product category we use. Luckily after defining all of the functions, it shouldn't be that hard. We really only need to do a simple steps.\n",
    "1. Load in a category of Amazon products as a dataframe\n",
    "2. Clean the dataframe\n",
    "3. Query the API and build historical price data csv\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m appliances \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "category = 'example_category'\n",
    "data_path = Path('data/keepa_data') / f'{category}.csv'\n",
    "example_category = pd.read_csv(data_path)\n",
    "example_category = clean_frame(example_category)\n",
    "query_keepa_in_batches(example_category, category, max_batches=10, batch_size=25, days = 1460, start_index = 0, end_index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CELSIUS Sparkling Oasis Vibe, Functional Essen...</td>\n",
       "      <td>energy drinks</td>\n",
       "      <td>B0C7FBH4QV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nespresso Chocolate Fudge Flavor VertuoLine po...</td>\n",
       "      <td>single-serve capsules &amp; pods</td>\n",
       "      <td>B08RJJ8VP7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premier Protein Shake, Winter Mint Chocolate, ...</td>\n",
       "      <td>protein drinks</td>\n",
       "      <td>B0CZ4KLN8Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparkling Ice, Berry Lemonade Sparkling Water,...</td>\n",
       "      <td>carbonated water</td>\n",
       "      <td>B0B192RX4X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monster Energy Ultra Violet, Sugar Free Energy...</td>\n",
       "      <td>energy drinks</td>\n",
       "      <td>B0BL6WMRGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>WATERMELON JOLLY RANCHER Hard Candy Original F...</td>\n",
       "      <td>hard candy</td>\n",
       "      <td>B0CCPPKG6B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Starbucks Premium Instant Coffee, Dark Roast, ...</td>\n",
       "      <td>instant coffee</td>\n",
       "      <td>B08YPDCZCN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Lipton Honey Ginger Green Tea Bags, Flavored, ...</td>\n",
       "      <td>green</td>\n",
       "      <td>B0D613P593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Luxardo Gourmet Cocktail Maraschino Cherries |...</td>\n",
       "      <td>cherries</td>\n",
       "      <td>B0CYP1ZS3K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Gatorade G Zero Powder, Fruit Punch, 0.10oz Pa...</td>\n",
       "      <td>sports drinks</td>\n",
       "      <td>B08R24D1KQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         product_title  \\\n",
       "0    CELSIUS Sparkling Oasis Vibe, Functional Essen...   \n",
       "1    Nespresso Chocolate Fudge Flavor VertuoLine po...   \n",
       "2    Premier Protein Shake, Winter Mint Chocolate, ...   \n",
       "3    Sparkling Ice, Berry Lemonade Sparkling Water,...   \n",
       "4    Monster Energy Ultra Violet, Sugar Free Energy...   \n",
       "..                                                 ...   \n",
       "995  WATERMELON JOLLY RANCHER Hard Candy Original F...   \n",
       "996  Starbucks Premium Instant Coffee, Dark Roast, ...   \n",
       "997  Lipton Honey Ginger Green Tea Bags, Flavored, ...   \n",
       "998  Luxardo Gourmet Cocktail Maraschino Cherries |...   \n",
       "999  Gatorade G Zero Powder, Fruit Punch, 0.10oz Pa...   \n",
       "\n",
       "                      subcategory        asin  \n",
       "0                   energy drinks  B0C7FBH4QV  \n",
       "1    single-serve capsules & pods  B08RJJ8VP7  \n",
       "2                  protein drinks  B0CZ4KLN8Q  \n",
       "3                carbonated water  B0B192RX4X  \n",
       "4                   energy drinks  B0BL6WMRGG  \n",
       "..                            ...         ...  \n",
       "995                    hard candy  B0CCPPKG6B  \n",
       "996                instant coffee  B08YPDCZCN  \n",
       "997                         green  B0D613P593  \n",
       "998                      cherries  B0CYP1ZS3K  \n",
       "999                 sports drinks  B08R24D1KQ  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'grocery_and_foods'\n",
    "data_path = Path('data/keepa_data') / f'{category}.csv'\n",
    "grocery_and_foods = pd.read_csv(data_path)\n",
    "grocery_and_foods = grocery_and_foods.rename(columns = {'Title' : 'product_title', 'Categories: Sub' : 'subcategory', 'ASIN' : 'asin'})\n",
    "grocery_and_foods = grocery_and_foods.apply(clean_data, axis = 1)\n",
    "grocery_and_foods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.81it/s]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:09<00:00,  2.02it/s]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.74it/s]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1177 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 1 seconds for additional tokens\n",
      "100%|██████████| 20/20 [19:50<00:00, 59.54s/it]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1190 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds for additional tokens\n",
      "100%|██████████| 20/20 [20:03<00:00, 60.18s/it]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1191 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds for additional tokens\n",
      "100%|██████████| 20/20 [20:04<00:00, 60.20s/it]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1191 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds for additional tokens\n",
      "100%|██████████| 20/20 [20:08<00:00, 60.44s/it]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1187 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds for additional tokens\n",
      "100%|██████████| 20/20 [19:58<00:00, 59.93s/it]\n",
      "/var/folders/tq/89nf6vzj1dv40hkl4p1k939r0000gn/T/ipykernel_25520/4197724147.py:48: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_avg = df.resample('M').mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 processed and appended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1192 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds for additional tokens\n"
     ]
    },
    {
     "ename": "ReadTimeout",
     "evalue": "HTTPSConnectionPool(host='api.keepa.com', port=443): Read timed out. (read timeout=10.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/http/client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/http/client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/http/client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/util/retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/util/util.py:39\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/connectionpool.py:538\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/urllib3/connectionpool.py:369\u001b[0m, in \u001b[0;36mHTTPConnectionPool._raise_timeout\u001b[0;34m(self, err, url, timeout_value)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(err, SocketTimeout):\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeoutError(\n\u001b[1;32m    370\u001b[0m         \u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead timed out. (read timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# See the above comment about EAGAIN in Python 3.\u001b[39;00m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='api.keepa.com', port=443): Read timed out. (read timeout=10.0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery_keepa_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrocery_and_foods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1460\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[154], line 28\u001b[0m, in \u001b[0;36mquery_keepa_in_batches\u001b[0;34m(products, category, max_batches, batch_size, days, start_index, stop_index)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_batches:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m df_batch \u001b[38;5;241m=\u001b[39m \u001b[43mget_monthly_avg_prices\u001b[49m\u001b[43m(\u001b[49m\u001b[43masin_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# If the CSV file exists, append without headers; otherwise, create a new file with headers.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_file):\n",
      "Cell \u001b[0;32mIn[94], line 24\u001b[0m, in \u001b[0;36mget_monthly_avg_prices\u001b[0;34m(asins, days)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03masins: list of ASIN strings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mdays: how many days of history to request (default 1460 ~ 4 years)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    - Values = average 'NEW' price for that month\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Single API call for all ASINs\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m products \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43masins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m products:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/keepa/interface.py:903\u001b[0m, in \u001b[0;36mKeepa.query\u001b[0;34m(self, items, stats, domain, history, offers, update, to_datetime, rating, out_of_stock_as_nan, stock, product_code_is_asin, progress_bar, buybox, wait, days, only_live_offers, raw, videos, aplus)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# request from keepa and increment current position\u001b[39;00m\n\u001b[1;32m    902\u001b[0m item_request \u001b[38;5;241m=\u001b[39m items[idx : idx \u001b[38;5;241m+\u001b[39m nrequest]  \u001b[38;5;66;03m# noqa: E203\u001b[39;00m\n\u001b[0;32m--> 903\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_product_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mitem_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduct_code_is_asin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrating\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_datetime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_of_stock_as_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_of_stock_as_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuybox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuybox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_live_offers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monly_live_offers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43maplus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maplus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nrequest\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raw:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/keepa/interface.py:1036\u001b[0m, in \u001b[0;36mKeepa._product_query\u001b[0;34m(self, items, product_code_is_asin, **kwargs)\u001b[0m\n\u001b[1;32m   1034\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwait\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1035\u001b[0m raw_response \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1036\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_response\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_response:\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/keepa/interface.py:1625\u001b[0m, in \u001b[0;36mKeepa._request\u001b[0;34m(self, request_type, payload, wait, raw_response)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwait_for_tokens()\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1625\u001b[0m     raw \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://api.keepa.com/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrequest_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1630\u001b[0m     status_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(raw\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[1;32m   1631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m200\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/dsc80/lib/python3.12/site-packages/requests/adapters.py:713\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[0;32m--> 713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ReadTimeout(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, _InvalidHeader):\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidHeader(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[0;31mReadTimeout\u001b[0m: HTTPSConnectionPool(host='api.keepa.com', port=443): Read timed out. (read timeout=10.0)"
     ]
    }
   ],
   "source": [
    "query_keepa_in_batches(grocery_and_foods, category, max_batches=25, batch_size=20, days = 1460, start_index = 0, stop_index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "The code of the cell above is mostly generated by ChatGPT from the prompt \"How can I load the Keepa API into an ipynb, and query price data every 30 days for the past 4 years?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #2 (if you have more than one, use name instead of number here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thoughtful discussion of ethical concerns included\n",
    "- Ethical concerns consider the whole data science process (question asked, data collected, data being used, the bias in data, analysis, post-analysis, etc.)\n",
    "- How your group handled bias/ethical concerns clearly described\n",
    "\n",
    "Acknowledge and address any ethics & privacy related issues of your question(s), proposed dataset(s), and/or analyses. Use the information provided in lecture to guide your group discussion and thinking. If you need further guidance, check out [Deon's Ethics Checklist](http://deon.drivendata.org/#data-science-ethics-checklist). In particular:\n",
    "\n",
    "- Are there any biases/privacy/terms of use issues with the data you propsed?\n",
    "- Are there potential biases in your dataset(s), in terms of who it composes, and how it was collected, that may be problematic in terms of it allowing for equitable analysis? (For example, does your data exclude particular populations, or is it likely to reflect particular human biases in a way that could be a problem?)\n",
    "- How will you set out to detect these specific biases before, during, and after/when communicating your analysis?\n",
    "- Are there any other issues related to your topic area, data, and/or analyses that are potentially problematic in terms of data privacy and equitable impact?\n",
    "- How will you handle issues you identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read over the [COGS108 Team Policies](https://github.com/COGS108/Projects/blob/master/COGS108_TeamPolicies.md) individually. Then, include your group’s expectations of one another for successful completion of your COGS108 project below. Discuss and agree on what all of your expectations are. Discuss how your team will communicate throughout the quarter and consider how you will communicate respectfully should conflicts arise. By including each member’s name above and by adding their name to the submission, you are indicating that you have read the COGS108 Team Policies, accept your team’s expectations below, and have every intention to fulfill them. These expectations are for your team’s use and benefit — they won’t be graded for their details.\n",
    "\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your team's specific project timeline. An example timeline has been provided. Changes the dates, times, names, and details to fit your group's plan.\n",
    "\n",
    "If you think you will need any special resources or training outside what we have covered in COGS 108 to solve your problem, then your proposal should state these clearly. For example, if you have selected a problem that involves implementing multiple neural networks, please state this so we can make sure you know what you’re doing and so we can point you to resources you will need to implement your project. Note that you are not required to use outside methods.\n",
    "\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM | Read & Think about COGS 108 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data (Ant Man); EDA (Hulk) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin Analysis (Iron Man; Thor) | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
