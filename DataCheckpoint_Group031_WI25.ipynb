{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you lost points on the last checkpoint you can get them back by responding to TA/IA feedback**  \n",
    "\n",
    "Update/change the relevant sections where you lost those points, make sure you respond on GitHub Issues to your TA/IA to call their attention to the changes you made here.\n",
    "\n",
    "Please update your Timeline... no battle plan survives contact with the enemy, so make sure we understand how your plans have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Ryan Lindberg\n",
    "- Nathan Mitchell\n",
    "- Domonick Marshall\n",
    "- Sean Notolli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"How have Amazon product prices across different market sectors changed between 2021-2024, and how do these trends compare to equivalent categories in the Consumer Price Index (CPI)? Is Amazon pricing in line with overall inflation, or does it diverge from broader economic trends?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Amazon has been a staple ecommerce service in many lives across the globe, bested by no other. Due to its broad market and utmost convenience, Amazon is one of the first markets considered when needing anything. However, one might wonder how costly this convenience is.\n",
    "\n",
    "From the Amazon Product Pricing Report 2024 on Issuu <a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1), we can see that Amazon prices are influenced by a vast amount of factors such as supply and demand, seasonal trends, competition, shifting seller fees, and algorithmic pricing. Other elements such as brand power, customer reviews, and holiday shopping behavior also contribute to pricing variability. This report serves as essential information for this project as it provides context to better interpret and analyze Amazon price trends. The report provides information on different pricing trends across different sectors of the market such as beauty, home and kitchen, arts and crafts, pet supply, and baby products, and demonstrates how each sector faces different trends. With each market sector, the report also produces concise and clear visualizations of pricing data across multiple Amazon products. \n",
    "\n",
    "The Consumer Price Index (CPI) Summary from the US Bureau of Labor Statistics <a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) offers guidance as to how to structure and analyze data related to our topic. Their report on CPI changes from 2023-2024 exemplifies how to organize large datasets and distill them into clear, actionable insights. The summary’s consistent formatting and emphasis on year-over-year percentage changes allow for a straightforward understanding of trends in consumer prices across different sectors. The structured approach will be instrumental in our own analysis of pricing data, helping us standardize our methodology and avoid potential misinterpretations. By adopting their organizational strategy, we can enhance the accuracy and credibility of our findings.\n",
    "\n",
    "Recent research has also talked about the idea of practical implications on dynamic algorithms in pricing on market behavior. Elmaghraby and Keskinocak <a name=\"cite_ref-3\"></a>[<sup>2</sup>](#cite_note-3) provide a review of dynamic pricing models explaining that factors like consumer demand and supply constraints drive pricing decisions in various industries. The paper talks about how algorithmic pricing is not only a tool for optimizing revenue but it can contribute to pricing volatility and competitive differences in the digital market. Using their findings with the data from Amazon product pricing report and CPI lets us understand how algorithmic strategies and external market conditions can interact with the price trends, Underscoring how important advanced computational methods are in predicting market behaviors in a retail environment. \n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Jungle Scout (2024, Jan). Amazon Product Pricing Report 2024. Issuu. https://issuu.com/junglescoutcobalt/docs/jungle-scout-amazon-product-pricing-report-2024?utm_source=chatgpt.html \n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Bureau of Labor Statistics (2024, Feb). Consumer Price Index Summary. U.S. Department of Labor. https://www.bls.gov/news.release/pdf/cpi.html\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3) Elmaghraby, W., & Keskinocak, P. (2003). Dynamic Pricing in the Presence of Inventory Considerations: Research Overview, Current Practices, and Future Directions. Management Science, 49(10), https://www.researchgate.net/publication/220534328_Dynamic_Pricing_in_the_Presence_of_Inventory_Considerations_Research_Overview_Current_Practices_and_Future_Directions.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Amazon prices across different market sectors have generally increased from 2021-2024 but at an inconsistent rate with inflation trends in the consumer price index. We predict that discretionary goods like electronics and other luxury items have had smaller price increases when compared to essential goods like groceries and other necessities potentially exceeding the CPI inflation rate. We think this is due to Amazon using aggressive price matching and algorithms to remain competitive in non-essential categories where supply chain constraints and labor costs disproportionately impacted essential goods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "- Dataset #1\n",
    "  - Dataset Name: Keepa\n",
    "  - Link to the dataset: https://keepa.com/#!data\n",
    "  - Number of observations: 998\n",
    "  - Number of variables: 63\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - Dataset Name: Consumer Price Index (CPI)\n",
    "  - Link to the dataset: https://www.bls.gov/cpi/tables/supplemental-files/ \n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "- etc\n",
    "\n",
    "Now write 2 - 5 sentences describing each dataset here. Include a short description of the important variables in the dataset; what the metrics and datatypes are, what concepts they may be proxies for. Include information about how you would need to wrangle/clean/preprocess the dataset\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Install the keepa library (run this cell if not already installed)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install keepa\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "# Install the keepa library (run this cell if not already installed)\n",
    "#!pip install keepa\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import keepa\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"df2mtauj1tmrngcm95ubshd41fplpf2bfh1nba8s8hpd2m6golbbrj9bat7osb8o\" # do no share outside of private repo!!\n",
    "api = keepa.Keepa(ACCESS_KEY, timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the price of a given amazon product, there are many different types of the the 'price' variable we can access. Two of them are new price and listing price. Here are the differences:\n",
    "\n",
    "• NEW PRICE:\n",
    "This is the current selling price for an item offered on Amazon in brand new condition. It reflects the actual market price that customers pay—typically the lowest available offer among Amazon and third‑party sellers. Because it’s influenced by promotions, competition, and real‑time market conditions, the NEW price can fluctuate over time.\n",
    "\n",
    "• LISTING PRICE:\n",
    "This is usually the manufacturer’s suggested retail price (MSRP) or the original price displayed on the product’s listing. It tends to be more stable and is often used as a reference to show discounts or price reductions. Even when the new price drops (for deals or competitive reasons), the listing price may remain unchanged.\n",
    "\n",
    "The new price shows you what you’d pay right now, while the listing price is a reference value set by the manufacturer. This difference helps sellers and buyers gauge the discount depth and market dynamics.\n",
    "\n",
    "Since we want to accurately capture the price a consumer is paying at a given time, we will use the NEW PRICE.\n",
    "\n",
    "Other price variables we can access are the NEW_FBM (Filled by Manufacturer), NEW_FBA (Filled by Amazon), USED, (used items), REFURBISHED (refurbished items), WAREHOUSE (prices from amazon warhouse deals, usually returned items). These price variables offer spotty data coverage, and don't accurately represent the consumer experience, so we will not be using them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Price History Dataset from Keepa API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to gather data for a variety of Amazon products from different sectors of the market. Luckily, each product on Amazon has a unique ASIN (Amazon Standard Identification Number) which we can use to identify it. With help from the Keepa Data Product Finder tool, we can collect these ASINs along with other product information. In the following example we use the Product Finder to filter by 'product category' = baby products, and rank = 1-1000. This finds the top 1000 items categorized under 'baby products' on Amazon. Additionally we can collect the name of the product, and the sub category. We made sure to exclude 'variations' of products, because we dont want to sample the same product 5 times just because there are 5 versions of the product with slight variations. We also refined our query to only physical products, excluding digital products and ebooks. CPI data is only collected on physical goods, so this allows us to have a fairer comparison. And our final filter is 'tracking time' which allows us to only select products that have been tracked by Keepa from 2021-2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# read in top 1000 baby products csv:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[43mPath\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/keepa_data\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrocery_and_foods.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m products \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "# read in top 1000 baby products csv:\n",
    "data_path = Path('data/keepa_data') / 'grocery_and_foods.csv'\n",
    "products = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Categories: Sub</th>\n",
       "      <th>ASIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CELSIUS Sparkling Strawberry Guava, Functional...</td>\n",
       "      <td>Energy Drinks, Gluten-Free Groceries, Evergree...</td>\n",
       "      <td>B08PGXDHTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nespresso Capsules Vertuo, Voltesso, Mild Roas...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Kitchen &amp; Dining...</td>\n",
       "      <td>B0768N9N6P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premier Protein Shake, Cookies &amp; Cream, 30g Pr...</td>\n",
       "      <td>Protein Drinks, Beverages, Protein drinks</td>\n",
       "      <td>B07MFYYZ5B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparkling Ice, Peach Nectarine Sparkling Water...</td>\n",
       "      <td>Carbonated Water, Subscribe &amp; Save Prime Promo...</td>\n",
       "      <td>B009S2XFVW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sparkling Ice, Cranberry Frost Sparkling Water...</td>\n",
       "      <td>Soft Drinks, Sparkling water, Sparkling Water,...</td>\n",
       "      <td>B07KY58NFX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monster Energy Zero Ultra, Sugar Free Energy D...</td>\n",
       "      <td>Energy Drinks, Subscribe &amp; Save Prime Promo, B...</td>\n",
       "      <td>B00ADYXY7E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Core Power Protein Shake, Chocolate, 26g Bottl...</td>\n",
       "      <td>Protein Drinks, Balanced Nutrition, Prime Memb...</td>\n",
       "      <td>B07LD2NV9X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Starbucks K-Cup Coffee Pods, Medium Roast Coff...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Packaged Coffee,...</td>\n",
       "      <td>B00U3ODTTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nespresso Capsules Vertuo, Double Espresso Scu...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Kitchen &amp; Dining...</td>\n",
       "      <td>B07M8YV12G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nespresso Capsules VertuoLine, Hazelino Muffin...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Packaged Coffee,...</td>\n",
       "      <td>B0851ZVCGL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  CELSIUS Sparkling Strawberry Guava, Functional...   \n",
       "1  Nespresso Capsules Vertuo, Voltesso, Mild Roas...   \n",
       "2  Premier Protein Shake, Cookies & Cream, 30g Pr...   \n",
       "3  Sparkling Ice, Peach Nectarine Sparkling Water...   \n",
       "4  Sparkling Ice, Cranberry Frost Sparkling Water...   \n",
       "5  Monster Energy Zero Ultra, Sugar Free Energy D...   \n",
       "6  Core Power Protein Shake, Chocolate, 26g Bottl...   \n",
       "7  Starbucks K-Cup Coffee Pods, Medium Roast Coff...   \n",
       "8  Nespresso Capsules Vertuo, Double Espresso Scu...   \n",
       "9  Nespresso Capsules VertuoLine, Hazelino Muffin...   \n",
       "\n",
       "                                     Categories: Sub        ASIN  \n",
       "0  Energy Drinks, Gluten-Free Groceries, Evergree...  B08PGXDHTC  \n",
       "1  Single-Serve Capsules & Pods, Kitchen & Dining...  B0768N9N6P  \n",
       "2          Protein Drinks, Beverages, Protein drinks  B07MFYYZ5B  \n",
       "3  Carbonated Water, Subscribe & Save Prime Promo...  B009S2XFVW  \n",
       "4  Soft Drinks, Sparkling water, Sparkling Water,...  B07KY58NFX  \n",
       "5  Energy Drinks, Subscribe & Save Prime Promo, B...  B00ADYXY7E  \n",
       "6  Protein Drinks, Balanced Nutrition, Prime Memb...  B07LD2NV9X  \n",
       "7  Single-Serve Capsules & Pods, Packaged Coffee,...  B00U3ODTTM  \n",
       "8  Single-Serve Capsules & Pods, Kitchen & Dining...  B07M8YV12G  \n",
       "9  Single-Serve Capsules & Pods, Packaged Coffee,...  B0851ZVCGL  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ASINs, product name, and subcategory, we've noticed that many products have multiple sub categories, most of which are irrelevant for the purposes of our analysis such as \"Christmas Store\", \"TEST ABCDEFGPD\" and even random characters such as \"d963aedb-8e7e-493c...\". We want to remove clean the subcategory column so that each product has a single subcategory. Luckily for us, it seems that the first sub category is the most descriptive subcategory of a given product, so we can remove the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categories: Sub\n",
       "Protein Drinks, Subscribe & Save Prime Promo, Spring Savings, New Year New You 2016, Organic Groceries, Grocery Back to School 17, Safe & Healthy Customer Favorites, Everyday grocery items, New Year, New You in Grocery, Grocery Subscribe and Save, Save up to 35% on Orgain Favorites, Grocery Subscribe & Save Event, Subscribe and Save for Back to School, Best Selling USDA Organic Products, Featured SNAP-eligible groceries, SNAP Beverages, d963aedb-8e7e-493c-80e7-b70e3c253824_9301, BTS B3G1 Grocery, Plant-Based Lifestyles, New Year New You - Healthy Eating, FREE Sample: Drazil Kids Tea, Punch Passion, Beverages, Back to School Everyday Essentials, Protein drinks    1\n",
       "Crackers, Snacks & produce IA, Crackers, Game day snacks & dips, Snacks, Snack favorites, Snacks Under 5$, LMP_California_Snacks, 15% off coupon terms and conditions, Gluten Free, Snacks, Keto Friendly Fats, Gluten-Free Groceries, Featured SNAP-eligible groceries, SNAP Crackers, SNAP Snacks, d963aedb-8e7e-493c-80e7-b70e3c253824_9301, Snacks, Snacks, Game Day Snacks, Canned, Packaged & Baking, OTC, Snacks under $5, ILM ASINs A, Hosting Hub, New to Fresh Snacks, Blue Diamond Almonds, Charcuterie, RFS Food, Snack Foods - Sugar-free, Shop & Save Terms & Conditions, Crackers and bread                                                                                     1\n",
       "Chewing & Bubble Gum, Candy & Chocolate, Restaurant and Bulk Food Supply, Featured SNAP-eligible groceries, Canned, Packaged & Baking, All Candy Coupons, National Candy Month, Movie Night Coupons, Snacks Candy, Sugar Free Gum, Gum and mints, Non-chocolate candy                                                                                                                                                                                                                                                                                                                                                                                                                          1\n",
       "Roasted Coffee Beans, MasterCard Subscribe & Save Show All, Christmas Store, Packaged Coffee, Cozy up with coffee, Coffee & Tea, MasterCard Subscribe & Save Beverages, Breakfast Deals, Coffee, Save on Coffee & Tea, Amazon Student Exclusive - 20% Off Groceries, d963aedb-8e7e-493c-80e7-b70e3c253824_787902, Beverages, OTC, Coffee & creamer, Main Coffee Base Shoveler, Coffee & Tea, Whole Beans                                                                                                                                                                                                                                                                                       1\n",
       "Sports Drinks, Refreshing beverages, Beverages, Quench your thirst, Featured SNAP-eligible groceries, SNAP Beverages, Beverages, Sports drinks, OTC, Sports Drinks, All Beverages, RFS Food, Springtime Beverages, Beverages for Summer                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "White, Pantry, Step 3. Build your base, Gluten-Free Groceries, Plant-Based Lifestyles                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1\n",
       "Gummy Candy, TEST ABCDEFGPD, Save 25% on Easter Candy and Gum, Back to School: Cookies & Treats, Candy & Chocolate, Featured SNAP-eligible groceries, Grocery Easter, Canned, Packaged & Baking, All Candy Coupons, Movie Night Coupons, Non-chocolate candy                                                                                                                                                                                                                                                                                                                                                                                                                                   1\n",
       "Peppercorns, Condiments & sauces, Pantry Staples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               1\n",
       "Snack & Trail Mixes, Snacks & produce IA, Paleo-friendly foods, Game day snacks & dips, Snacks, Snack favorites, Salty snacks, Snacks Under 5$, 15% off coupon terms and conditions, Snacks, Featured SNAP-eligible groceries, Snacks, Snacks, Plant-Based Lifestyles, Game Day Snacks, Canned, Packaged & Baking, OTC, Snacks under $5, Chips and Snacks, New to Fresh Snacks, Snacks, RFS Food, Other cheese pairings, Nuts & Trail Mix, Shop & Save Terms & Conditions                                                                                                                                                                                                                      1\n",
       "Gummy Candy, Candy & Chocolate, Gluten-Free Groceries, SNAP_Sweets, Canned, Packaged & Baking, All Candy Coupons, Easter Selection, National Candy Month, Ready for School, Snacks, Condiments, Baking, $1, Gummy candy, Movie Night Coupons, DCA2 - Sweet Snacks, Non-chocolate candy                                                                                                                                                                                                                                                                                                                                                                                                         1\n",
       "Sports Drinks, Refreshing beverages, Beverages, Quench your thirst, Beverages, Beverages, Memorial Day Coupons, Sports drinks, OTC, Sports Drinks, Save up to 20% on Select Gatorade Products, Ready for School, Fresh Grocery Coupons, Drinks - Sandwich Store, Ad-Leftt-Bulk- Beverage, Beverages, RFS Food, Kickoff to summer, Springtime Beverages, Beverages for Summer, Drinks                                                                                                                                                                                                                                                                                                           1\n",
       "Meat & Seafood, Meat & Seafood, NYNY Snacks, APS Test, Meat & seafood, Featured SNAP-eligible groceries, SNAP Snacks, Fresh, Quick Meals & Snacks, Canned, Packaged & Baking, Memorial Day Coupons, OTC, Eat In ASINS, Jack Links, RFS Food, Snacks, Meat & Seafood                                                                                                                                                                                                                                                                                                                                                                                                                            1\n",
       "Tuna Fish, Pantry, Pack in protein, Canned Tuna & Seafood, ILM ASINs A, Pantry Staples, Specialty Diets_keto (total), Deli Meats & Tuna - Sandwich Store, Pantry staples, SANDWICH STORE, Keto                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 1\n",
       "Single-Serve Capsules & Pods, Packaged Coffee, Cozy up with coffee, Coffee & Tea, Coffee, Save on Coffee & Tea, 1, Beverages, OTC, Coffee & creamer, Main Coffee Base Shoveler, Coffee & Tea, Cups/Pods                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "Dried Dates, Salad toppings, Snacks & produce IA, Game day snacks & dips, Snacks, APS Test, Safe & Healthy Customer Favorites, Snacks Under 5$, 15% off coupon terms and conditions, Salads, Fresh produce, Snacks, Nuts, Seeds, Dried Fruits & Vegetables, Fresh, Game Day Snacks, Canned, Packaged & Baking, OTC, Snacks under $5, Fresh and dried fruit, Deals_FreshProduce, New to Fresh Snacks, RFS Food, Fruit Snacks & Dried Fruit, Shop & Save Terms & Conditions                                                                                                                                                                                                                      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['Categories: Sub'].value_counts().tail(15) # bottom 15 subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function remove extra subcategories from each row and rename columns\n",
    "def clean_frame(df):\n",
    "    df = df.rename(columns = {'Title' : 'product_title', 'Categories: Sub' : 'subcategory', 'ASIN' : 'asin'})\n",
    "    def clean_row(row):\n",
    "        row['subcategory'] = row['subcategory'].split(',')[0].strip().lower()\n",
    "        return row\n",
    "    df = df.apply(clean_row, axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "products = clean_frame(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subcategory\n",
       "cooking & baking      1\n",
       "frozen                1\n",
       "herbs                 1\n",
       "vegetables            1\n",
       "potato side dishes    1\n",
       "baking powder         1\n",
       "canola                1\n",
       "bitters               1\n",
       "frosting              1\n",
       "chile paste           1\n",
       "mixed                 1\n",
       "brown sugar           1\n",
       "ground pepper         1\n",
       "fruit                 1\n",
       "crackers              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['subcategory'].value_counts().tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['subcategory'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our least common subcategories are actually meaningful, and we are left with 179 subcategories for baby products.\n",
    "\n",
    "\n",
    "Moving forward, we can use the ASINs column to query the Keepa API for historical price data of each item.\n",
    "We define multiple functions below to achieve this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. keepa_time_to_datetime(kt):**\n",
    "\n",
    "This function takes in a keepa time integer, and converts it to standard time. Each price value has an attached time value so we know when that price was in effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. def generate_monthly_headers(days):**\n",
    "\n",
    "This function allows us to standardize which months we actually want to collect data for. When we query Keepa, it may return data from 2011 if the data is available. We dont want that data. So we can use this function to ensure that we filter the data for the months we need. It is also helpful in ensuring every time we collect data within a given range, the column headers are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. def get_monthly_avg_prices(asins, days):**\n",
    "\n",
    "This function gathers historical price data for a list of Amazon products using their ASINs. It returns a dataframe of the average price from the previous x amount of days for each product. We want data since 2020, so we will be using (365 * 5) for our days variable. It has some special features such as forward filling, which is helpful to fill in missing data. Since Keepa only updates prices when the price changes, and some months don't have any price changes, the natural solution is to forward fill to fill in missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our basic membership access to the Keepa API, we are limited with how much data we can request, so we will have to produce our price data in batches, and incrementally build up a large csv of price data for different product categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. def batch(iterable, n=20):**\n",
    "\n",
    "This takes in a list of ASINs, and returns batches of them of size n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**5. def query_keepa_in_batches(products, category, max_batches=10, batch_size=20, days=365 * 4, start_index=0, stop_index=None):**\n",
    "\n",
    "To put it simply, this function just gets monthly avg price data for a batch of ASINs, then merges it to a master csv.\n",
    "\n",
    "With the function query_keepa_in_batches, we can incrementally build a csv of historical price data for a given category by repeatedly requesting data from the Keepa API. The inputs to this function are: a dataframe such as grocery_and_foods, the category we are working with, i.e. the string 'grocery_and_foods', the number of batches we want, batch size, how many days of historical price data we want, and the start and stop indices of the ASINs we want to query from the dataframe. These start and stop indices allow us to pick up where we left off. So if for some reason the query stops, we can just figure out the last batch we completed, and keep going from there! If the CSV already exists, we just append to it. If it does not exist, we create a new one. This ensures we never overwrite data, and we can always keep track of where we are in the request process. This function takes many hours to run since we are limited to 1 token per minute with our API membership, and each ASIN costs 1 token. So it will have to run overnight to collect our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepa_time_to_datetime(kt):\n",
    "    # Convert Keepa time (minutes since 2011-01-01) to a Python datetime (UTC)\n",
    "    if isinstance(kt, datetime.datetime):\n",
    "        return kt\n",
    "    return datetime.datetime.fromtimestamp((kt + 21564000) * 60, datetime.timezone.utc)\n",
    "\n",
    "def generate_monthly_headers(days):\n",
    "    \"\"\"\n",
    "    Generate a list of month headers (strings) in the format 'YYYY-MM'\n",
    "    spanning from the current month back to the month that includes (now - days).\n",
    "    The headers are in ascending order. 2021-2025\n",
    "    This function standardizes which months we collect for each batch and ensures the columns are aligned.\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    start_date = now - datetime.timedelta(days=days)\n",
    "    \n",
    "    headers = []\n",
    "    current_year = now.year\n",
    "    current_month = now.month\n",
    "\n",
    "    while True:\n",
    "        header = f\"{current_year:04d}-{current_month:02d}\"\n",
    "        headers.append(header)\n",
    "        # Move to the previous month\n",
    "        if current_month == 1:\n",
    "            current_month = 12\n",
    "            current_year -= 1\n",
    "        else:\n",
    "            current_month -= 1\n",
    "        \n",
    "        # Create a timezone-aware date for the first day of the new month.\n",
    "        month_start = datetime.datetime(current_year, current_month, 1, tzinfo=datetime.timezone.utc)\n",
    "        # Stop if this month is before the start_date.\n",
    "        if month_start < start_date:\n",
    "            break\n",
    "    return sorted(headers)\n",
    "\n",
    "\n",
    "def get_monthly_avg_prices(asins, days=1460):\n",
    "    \"\"\"\n",
    "    asins: list of ASIN strings\n",
    "    days: number of days of history (default 1460 ~ 4 years)\n",
    "    \n",
    "    Returns a DataFrame:\n",
    "        - Rows = ASINs\n",
    "        - Columns = monthly time periods (e.g. '2025-02', '2025-01', etc.)\n",
    "        - Values = average 'NEW' price for that month\n",
    "    \"\"\"\n",
    "    products = api.query(asins, days=days)\n",
    "    dfs = []\n",
    "    for product in products:\n",
    "        asin = product['asin']\n",
    "        price_history = product['data'].get('NEW', [])\n",
    "        time_history  = product['data'].get('NEW_time', [])\n",
    "        \n",
    "        dates = [keepa_time_to_datetime(t) for t in time_history]\n",
    "        prices = [p for p in price_history]\n",
    "        \n",
    "        df = pd.DataFrame({'date': dates, asin: prices})\n",
    "        df.set_index('date', inplace=True)\n",
    "        \n",
    "        # Resample to monthly average using month-end frequency\n",
    "        monthly_avg = df.resample('ME').mean()\n",
    "        dfs.append(monthly_avg)\n",
    "    \n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine and transpose so that rows = ASIN and columns = dates\n",
    "    combined = pd.concat(dfs, axis=1).T\n",
    "    # Convert datetime columns to string format 'YYYY-MM'\n",
    "    combined.columns = [col.strftime('%Y-%m') for col in combined.columns]\n",
    "    \n",
    "    # Generate the complete set of monthly headers (headers are in descending order)\n",
    "    headers = generate_monthly_headers(days)\n",
    "    \n",
    "    # Reindex with headers generated headers\n",
    "    combined = combined.reindex(columns=headers, fill_value=np.nan)\n",
    "    \n",
    "    # Forward fill missing values along the row (in chronological order)\n",
    "    combined = combined.ffill(axis=1)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def batch(iterable, n=20):\n",
    "    \"\"\"Yield successive n-sized chunks from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2021-03</th>\n",
       "      <th>2021-04</th>\n",
       "      <th>2021-05</th>\n",
       "      <th>2021-06</th>\n",
       "      <th>2021-07</th>\n",
       "      <th>2021-08</th>\n",
       "      <th>2021-09</th>\n",
       "      <th>2021-10</th>\n",
       "      <th>2021-11</th>\n",
       "      <th>2021-12</th>\n",
       "      <th>...</th>\n",
       "      <th>2024-05</th>\n",
       "      <th>2024-06</th>\n",
       "      <th>2024-07</th>\n",
       "      <th>2024-08</th>\n",
       "      <th>2024-09</th>\n",
       "      <th>2024-10</th>\n",
       "      <th>2024-11</th>\n",
       "      <th>2024-12</th>\n",
       "      <th>2025-01</th>\n",
       "      <th>2025-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B009S2XFVW</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.32358</td>\n",
       "      <td>14.412791</td>\n",
       "      <td>7.844828</td>\n",
       "      <td>7.844828</td>\n",
       "      <td>10.955</td>\n",
       "      <td>8.99</td>\n",
       "      <td>8.520645</td>\n",
       "      <td>8.169459</td>\n",
       "      <td>8.169459</td>\n",
       "      <td>...</td>\n",
       "      <td>11.99</td>\n",
       "      <td>11.99</td>\n",
       "      <td>11.7</td>\n",
       "      <td>14.995</td>\n",
       "      <td>13.163333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.976</td>\n",
       "      <td>14.976</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            2021-03   2021-04    2021-05   2021-06   2021-07  2021-08  \\\n",
       "B009S2XFVW      NaN  13.32358  14.412791  7.844828  7.844828   10.955   \n",
       "\n",
       "            2021-09   2021-10   2021-11   2021-12  ...  2024-05  2024-06  \\\n",
       "B009S2XFVW     8.99  8.520645  8.169459  8.169459  ...    11.99    11.99   \n",
       "\n",
       "            2024-07  2024-08    2024-09  2024-10  2024-11  2024-12  2025-01  \\\n",
       "B009S2XFVW     11.7   14.995  13.163333     15.0   14.976   14.976     12.0   \n",
       "\n",
       "            2025-02  \n",
       "B009S2XFVW     12.0  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMPLE USAGE of get_monthly_avg_prices\n",
    "asins = [\"B009S2XFVW\"]  \n",
    "monthly_prices = get_monthly_avg_prices(asins, days = 365 * 4)\n",
    "monthly_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_keepa_in_batches(products, category, max_batches=10, batch_size=20,\n",
    "                           days=365 * 4, start_index=0, stop_index=None):\n",
    "    \"\"\"\n",
    "    Query Keepa for monthly average prices in batches and incrementally save \n",
    "    the *merged* results (column-aligned) to a CSV file.\n",
    "    \"\"\"\n",
    "    # Slice the ASIN list based on start_index and stop_index.\n",
    "    asins = list(products['asin'])[start_index:stop_index]\n",
    "    csv_file = f'data/{category}_monthly_prices.csv'\n",
    "    for i, asin_batch in enumerate(batch(asins, batch_size)):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        df_batch = get_monthly_avg_prices(asin_batch, days=days)\n",
    "        if df_batch.empty:\n",
    "            print(f\"Batch {i+1} returned no data; skipping.\")\n",
    "            continue\n",
    "        # If the CSV file exists, read it, merge columns, then write back\n",
    "        if os.path.exists(csv_file):\n",
    "            existing_df = pd.read_csv(csv_file, index_col=0)\n",
    "            \n",
    "            # Merge on row index (ASIN) and combine columns (outer join).\n",
    "            # combine_first() will fill missing entries in existing_df with df_batch values.\n",
    "            merged_df = existing_df.combine_first(df_batch)\n",
    "            \n",
    "            # Ensure columns are in the correct order (descending monthly headers).\n",
    "            # This step uses the same monthly headers function to reindex.\n",
    "            headers = generate_monthly_headers(days)\n",
    "            merged_df = merged_df.reindex(columns=headers, fill_value=np.nan)\n",
    "            \n",
    "            merged_df.to_csv(csv_file, index=True)\n",
    "        else:\n",
    "            # If no CSV yet, just write df_batch as the first chunk\n",
    "            df_batch.to_csv(csv_file, index=True)\n",
    "        \n",
    "        print(f\"Batch {i+1} processed and merged.\")\n",
    "\n",
    "\n",
    "# Example helper function to split the ASIN list into batches.\n",
    "def batch(iterable, n=20): # 20 is default batch size\n",
    "    \"\"\"Yield successive n-sized chunks from iterable\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the boring part, waiting for the API requests as we slowly build up our csv. This code will take several hours to run as we have to accumulate tokens throughout the API request. It will probably run in the background and overnight.\n",
    "\n",
    "Example:\n",
    "query_keepa_in_batches(products, 'baby_products', batch_size = 20, max_batches=10, days = 365*2, start_index = 0, stop_index = None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entire process was just for the baby_products category. We will have to do this same process for every other product category we use. Luckily after defining all of the functions, it shouldn't be that hard. We only need to do a couple simple steps:\n",
    "1. Load in a category of Amazon products as a dataframe\n",
    "2. Clean the dataframe\n",
    "3. Query the API to build historical price data csv\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual use: \n",
    "category = 'grocery_and_foods'\n",
    "data_path = Path('data/keepa_data') / f'{category}.csv'\n",
    "grocery_and_foods = pd.read_csv(data_path)\n",
    "grocery_and_foods = clean_frame(grocery_and_foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1175 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds for additional tokens\n",
      "100%|██████████| 20/20 [19:49<00:00, 59.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 processed and merged.\n"
     ]
    }
   ],
   "source": [
    "# Actual use:\n",
    "# This code may run for multiple hours. 1 minute for each product queried. \n",
    "query_keepa_in_batches(grocery_and_foods, category, max_batches=5, batch_size=20, days = (365 * 5) + 60, start_index = 900, stop_index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "The code of the cells above are mostly generated by ChatGPT from the prompt \"How can I load the Keepa API into an ipynb, and query price data every 30 days for the past 4 years?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both the products and their prices, lets combine the two dataframes and do a final tidying of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>2020-01</th>\n",
       "      <th>2020-02</th>\n",
       "      <th>2020-03</th>\n",
       "      <th>2020-04</th>\n",
       "      <th>2020-05</th>\n",
       "      <th>2020-06</th>\n",
       "      <th>2020-07</th>\n",
       "      <th>2020-08</th>\n",
       "      <th>2020-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2024-05</th>\n",
       "      <th>2024-06</th>\n",
       "      <th>2024-07</th>\n",
       "      <th>2024-08</th>\n",
       "      <th>2024-09</th>\n",
       "      <th>2024-10</th>\n",
       "      <th>2024-11</th>\n",
       "      <th>2024-12</th>\n",
       "      <th>2025-01</th>\n",
       "      <th>2025-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000E5JIU</td>\n",
       "      <td>6.461333</td>\n",
       "      <td>6.522500</td>\n",
       "      <td>5.575000</td>\n",
       "      <td>5.575000</td>\n",
       "      <td>7.614286</td>\n",
       "      <td>7.365000</td>\n",
       "      <td>6.297500</td>\n",
       "      <td>5.572500</td>\n",
       "      <td>5.865833</td>\n",
       "      <td>...</td>\n",
       "      <td>9.386000</td>\n",
       "      <td>9.386000</td>\n",
       "      <td>9.485000</td>\n",
       "      <td>9.522727</td>\n",
       "      <td>9.011000</td>\n",
       "      <td>9.777143</td>\n",
       "      <td>9.732500</td>\n",
       "      <td>9.732500</td>\n",
       "      <td>9.817500</td>\n",
       "      <td>10.024286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0001UXQ9Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.232500</td>\n",
       "      <td>23.864118</td>\n",
       "      <td>11.935000</td>\n",
       "      <td>11.906154</td>\n",
       "      <td>11.318182</td>\n",
       "      <td>10.860000</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.285000</td>\n",
       "      <td>19.070714</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>13.796667</td>\n",
       "      <td>18.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0002LD9IW</td>\n",
       "      <td>7.810000</td>\n",
       "      <td>5.578182</td>\n",
       "      <td>4.962500</td>\n",
       "      <td>4.032000</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>3.818750</td>\n",
       "      <td>...</td>\n",
       "      <td>1.353333</td>\n",
       "      <td>1.353333</td>\n",
       "      <td>1.353333</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.032000</td>\n",
       "      <td>4.032000</td>\n",
       "      <td>4.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00032BPCM</td>\n",
       "      <td>3.847778</td>\n",
       "      <td>3.553333</td>\n",
       "      <td>7.652326</td>\n",
       "      <td>4.737206</td>\n",
       "      <td>1.565556</td>\n",
       "      <td>1.565556</td>\n",
       "      <td>1.565556</td>\n",
       "      <td>1.637500</td>\n",
       "      <td>3.287568</td>\n",
       "      <td>...</td>\n",
       "      <td>1.588947</td>\n",
       "      <td>2.105714</td>\n",
       "      <td>5.160645</td>\n",
       "      <td>4.680000</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>4.951667</td>\n",
       "      <td>5.269259</td>\n",
       "      <td>5.179375</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>5.057308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000AXW9XI</td>\n",
       "      <td>10.208286</td>\n",
       "      <td>11.024000</td>\n",
       "      <td>11.477500</td>\n",
       "      <td>14.070645</td>\n",
       "      <td>12.530238</td>\n",
       "      <td>11.881081</td>\n",
       "      <td>11.717692</td>\n",
       "      <td>11.456486</td>\n",
       "      <td>10.265370</td>\n",
       "      <td>...</td>\n",
       "      <td>10.024706</td>\n",
       "      <td>10.461000</td>\n",
       "      <td>11.595000</td>\n",
       "      <td>17.990000</td>\n",
       "      <td>17.980556</td>\n",
       "      <td>17.970000</td>\n",
       "      <td>18.993333</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>14.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>B08QR1MWHF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.690000</td>\n",
       "      <td>22.180000</td>\n",
       "      <td>22.886000</td>\n",
       "      <td>19.423333</td>\n",
       "      <td>19.423333</td>\n",
       "      <td>18.924706</td>\n",
       "      <td>17.882000</td>\n",
       "      <td>18.787143</td>\n",
       "      <td>18.394444</td>\n",
       "      <td>19.733571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>B08QTTJ1NM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>8.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>B08R4K3LR6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>41.980000</td>\n",
       "      <td>41.980000</td>\n",
       "      <td>46.381053</td>\n",
       "      <td>45.826667</td>\n",
       "      <td>43.594762</td>\n",
       "      <td>45.162727</td>\n",
       "      <td>45.162727</td>\n",
       "      <td>45.162727</td>\n",
       "      <td>44.874444</td>\n",
       "      <td>46.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>B08R6DZXYV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>13.864000</td>\n",
       "      <td>13.864000</td>\n",
       "      <td>13.036667</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>14.405000</td>\n",
       "      <td>13.130000</td>\n",
       "      <td>12.920000</td>\n",
       "      <td>12.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>B08RD41V4Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.610000</td>\n",
       "      <td>15.305000</td>\n",
       "      <td>13.984444</td>\n",
       "      <td>13.984444</td>\n",
       "      <td>12.492500</td>\n",
       "      <td>19.950000</td>\n",
       "      <td>13.985000</td>\n",
       "      <td>13.985000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    2020-01    2020-02    2020-03    2020-04    2020-05  \\\n",
       "0    B0000E5JIU   6.461333   6.522500   5.575000   5.575000   7.614286   \n",
       "1    B0001UXQ9Q        NaN        NaN  22.232500  23.864118  11.935000   \n",
       "2    B0002LD9IW   7.810000   5.578182   4.962500   4.032000   4.756667   \n",
       "3    B00032BPCM   3.847778   3.553333   7.652326   4.737206   1.565556   \n",
       "4    B000AXW9XI  10.208286  11.024000  11.477500  14.070645  12.530238   \n",
       "..          ...        ...        ...        ...        ...        ...   \n",
       "993  B08QR1MWHF        NaN        NaN        NaN        NaN        NaN   \n",
       "994  B08QTTJ1NM        NaN        NaN        NaN        NaN        NaN   \n",
       "995  B08R4K3LR6        NaN        NaN        NaN        NaN        NaN   \n",
       "996  B08R6DZXYV        NaN        NaN        NaN        NaN        NaN   \n",
       "997  B08RD41V4Y        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "       2020-06    2020-07    2020-08    2020-09  ...    2024-05    2024-06  \\\n",
       "0     7.365000   6.297500   5.572500   5.865833  ...   9.386000   9.386000   \n",
       "1    11.906154  11.318182  10.860000  10.650000  ...  16.285000  19.070714   \n",
       "2     4.756667   4.756667   4.756667   3.818750  ...   1.353333   1.353333   \n",
       "3     1.565556   1.565556   1.637500   3.287568  ...   1.588947   2.105714   \n",
       "4    11.881081  11.717692  11.456486  10.265370  ...  10.024706  10.461000   \n",
       "..         ...        ...        ...        ...  ...        ...        ...   \n",
       "993        NaN        NaN        NaN        NaN  ...  20.690000  22.180000   \n",
       "994        NaN        NaN        NaN        NaN  ...  24.990000  24.990000   \n",
       "995        NaN        NaN        NaN        NaN  ...  41.980000  41.980000   \n",
       "996        NaN        NaN        NaN        NaN  ...  13.270000  13.270000   \n",
       "997        NaN        NaN        NaN        NaN  ...  15.610000  15.305000   \n",
       "\n",
       "       2024-07    2024-08    2024-09    2024-10    2024-11    2024-12  \\\n",
       "0     9.485000   9.522727   9.011000   9.777143   9.732500   9.732500   \n",
       "1    11.625000  11.625000  11.625000  11.625000  12.800000  12.800000   \n",
       "2     1.353333   4.035000   4.035000   4.050000   4.050000   4.032000   \n",
       "3     5.160645   4.680000   5.980000   4.951667   5.269259   5.179375   \n",
       "4    11.595000  17.990000  17.980556  17.970000  18.993333  19.990000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "993  22.886000  19.423333  19.423333  18.924706  17.882000  18.787143   \n",
       "994  24.990000  24.990000   8.990000  14.990000  14.990000  14.990000   \n",
       "995  46.381053  45.826667  43.594762  45.162727  45.162727  45.162727   \n",
       "996  13.864000  13.864000  13.036667  13.270000  14.405000  13.130000   \n",
       "997  13.984444  13.984444  12.492500  19.950000  13.985000  13.985000   \n",
       "\n",
       "       2025-01    2025-02  \n",
       "0     9.817500  10.024286  \n",
       "1    13.796667  18.970000  \n",
       "2     4.032000   4.032000  \n",
       "3     5.166667   5.057308  \n",
       "4    16.180000  14.240000  \n",
       "..         ...        ...  \n",
       "993  18.394444  19.733571  \n",
       "994  14.990000  14.990000  \n",
       "995  44.874444  46.166667  \n",
       "996  12.920000  12.570000  \n",
       "997  14.990000  14.990000  \n",
       "\n",
       "[998 rows x 63 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'grocery_and_foods'\n",
    "data_path2 = Path('data/') / f'{category}_monthly_prices.csv'\n",
    "price_data = pd.read_csv(data_path2)\n",
    "\n",
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like during the gathering process we accidentally missed 2 rows but thats okay. Also something to notice is that rows near the end of the data tend to have less data in the earlier months. We suspect this is due to how Keepa sorts their ASINs. This shouldnt be too big of an issue though. With an extra years worth of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #2 Consumer Price Index (CPI) for All Urban Consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is a list of all separate data sets that we retreived from the CPI database.\n",
    "\n",
    "- [All items in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SA0)\n",
    "- [All items in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SA0)\n",
    "- [Food and beverages in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SAF)\n",
    "- [Food in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SAF1)\n",
    "- [Prescription drugs in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SEMF01)\n",
    "- [Commodities in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAC)\n",
    "- [Durables in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAD)\n",
    "- [Nondurables in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAN)\n",
    "- [Recreation in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAR)\n",
    "- [Appliances in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SEHK)\n",
    "- [Toys in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SERE01)\n",
    "- [Apparel in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAA)\n",
    "\n",
    "Notes:\n",
    "- Seasonally adjusted indicates that data has been statistically modified (by the CPI) to remove the effects of predictable seasonal fluctuations, like holiday shopping sprees or summer vacation trends, allowing for a clearer analysis of underlying trends without the influence of recurring seasonal patterns.\n",
    "- Durables are essentially products that are designed to last a long time and be purhcased rather infrequently.\n",
    "Ex: Cars, refrigerators, furniture, washing machines and dryers, musical instruments, etc.\n",
    "- Nondurables are goods that are generally consumed quickly and purchased more frequently. They lose value after one use and/or after a short period of time. Ex: food, drinks, hygiene products, paper products, cosmetics, clothing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'all_items_NSA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# All items in U.S. city average, all urban consumers, not seasonally adjusted\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m all_items_NSA \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_items_NSA.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m all_items_NSA \u001b[38;5;241m=\u001b[39m all_items_NSA\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeries ID\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m all_items_NSA \u001b[38;5;241m=\u001b[39m all_items_NSA\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPeriod\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'all_items_NSA.csv'"
     ]
    }
   ],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n",
    "## FEEL FREE TO ADD MULTIPLE CELLS PER SECTION\n",
    "\n",
    "#pip install pandas\n",
    "\n",
    "# All items in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "all_items_NSA = pd.read_csv('all_items_NSA.csv')\n",
    "all_items_NSA = all_items_NSA.drop(columns=['Series ID'])\n",
    "all_items_NSA = all_items_NSA.drop(columns=['Period'])\n",
    "all_items_NSA['Month'] = all_items_NSA['Label'].apply(lambda x: x.split()[1])\n",
    "all_items_NSA = all_items_NSA.drop(columns=['Label'])\n",
    "all_items_NSA = all_items_NSA[['Year', 'Month', 'Value']]\n",
    "\n",
    "\n",
    "# All items in U.S. city average, all urban consumers, seasonally adjusted\n",
    "all_items_SA = pd.read_csv('all_items_SA.csv')\n",
    "all_items_SA = all_items_SA.drop(columns=['Series ID'])\n",
    "all_items_SA = all_items_SA.drop(columns=['Period'])\n",
    "all_items_SA['Month'] = all_items_SA['Label'].apply(lambda x: x.split()[1])\n",
    "all_items_SA = all_items_SA.drop(columns=['Label'])\n",
    "all_items_SA = all_items_SA[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Food and beverages in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "food_and_bev = pd.read_csv('food_and_bev.csv')\n",
    "food_and_bev = food_and_bev.drop(columns=['Series ID'])\n",
    "food_and_bev = food_and_bev.drop(columns=['Period'])\n",
    "food_and_bev['Month'] = food_and_bev['Label'].apply(lambda x: x.split()[1])\n",
    "food_and_bev = food_and_bev.drop(columns=['Label'])\n",
    "food_and_bev = food_and_bev[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Food in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "food = pd.read_csv('just_food.csv')\n",
    "food = food.drop(columns=['Series ID'])\n",
    "food = food.drop(columns=['Period'])\n",
    "food['Month'] = food['Label'].apply(lambda x: x.split()[1])\n",
    "food = food.drop(columns=['Label'])\n",
    "food = food[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Prescription drugs in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "presc_drugs = pd.read_csv('presc_drugs.csv')\n",
    "presc_drugs = presc_drugs.drop(columns=['Series ID'])\n",
    "presc_drugs = presc_drugs.drop(columns=['Period'])\n",
    "presc_drugs['Month'] = presc_drugs['Label'].apply(lambda x: x.split()[1])\n",
    "presc_drugs = presc_drugs.drop(columns=['Label'])\n",
    "presc_drugs = presc_drugs[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Commodities in U.S. city average, all urban consumers, seasonally adjusted\n",
    "commodities = pd.read_csv('commodities.csv')\n",
    "commodities = commodities.drop(columns=['Series ID'])\n",
    "commodities = commodities.drop(columns=['Period'])\n",
    "commodities['Month'] = commodities['Label'].apply(lambda x: x.split()[1])\n",
    "commodities = commodities.drop(columns=['Label'])\n",
    "commodities = commodities[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Durables in U.S. city average, all urban consumers, seasonally adjusted\n",
    "durables = pd.read_csv('durables.csv')\n",
    "durables = durables.drop(columns=['Series ID'])\n",
    "durables = durables.drop(columns=['Period'])\n",
    "durables['Month'] = durables['Label'].apply(lambda x: x.split()[1])\n",
    "durables = durables.drop(columns=['Label'])\n",
    "durables = durables[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Nondurables in U.S. city average, all urban consumers, seasonally adjusted\n",
    "nondurables = pd.read_csv('nondurables.csv')\n",
    "nondurables = nondurables.drop(columns=['Series ID'])\n",
    "nondurables = nondurables.drop(columns=['Period'])\n",
    "nondurables['Month'] = nondurables['Label'].apply(lambda x: x.split()[1])\n",
    "nondurables = nondurables.drop(columns=['Label'])\n",
    "nondurables = nondurables[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Recreation in U.S. city average, all urban consumers, seasonally adjusted\n",
    "rec = pd.read_csv('recreation.csv')\n",
    "rec = rec.drop(columns=['Series ID'])\n",
    "rec = rec.drop(columns=['Period'])\n",
    "rec['Month'] = rec['Label'].apply(lambda x: x.split()[1])\n",
    "rec = rec.drop(columns=['Label'])\n",
    "rec = rec[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Appliances in U.S. city average, all urban consumers, seasonally adjusted\n",
    "appliances = pd.read_csv('appliances.csv')\n",
    "appliances = appliances.drop(columns=['Series ID'])\n",
    "appliances = appliances.drop(columns=['Period'])\n",
    "appliances['Month'] = appliances['Label'].apply(lambda x: x.split()[1])\n",
    "appliances = appliances.drop(columns=['Label'])\n",
    "appliances = appliances[['Year', 'Month', 'Value']]\n",
    "\n",
    "\n",
    "# Toys in U.S. city average, all urban consumers, seasonally adjusted\n",
    "toys = pd.read_csv('toys.csv')\n",
    "toys = toys.drop(columns=['Series ID'])\n",
    "toys = toys.drop(columns=['Period'])\n",
    "toys['Month'] = toys['Label'].apply(lambda x: x.split()[1])\n",
    "toys = toys.drop(columns=['Label'])\n",
    "toys = toys[['Year', 'Month', 'Value']]\n",
    "\n",
    "\n",
    "# Apparel in U.S. city average, all urban consumers, seasonally adjusted\n",
    "apparel = pd.read_csv('apparel.csv')\n",
    "apparel = apparel.drop(columns=['Series ID'])\n",
    "apparel = apparel.drop(columns=['Period'])\n",
    "apparel['Month'] = apparel['Label'].apply(lambda x: x.split()[1])\n",
    "apparel = apparel.drop(columns=['Label'])\n",
    "apparel = apparel[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Combine dataframes\n",
    "all_cpi = pd.DataFrame(columns = ['Year', 'Month', 'all_items_NSA', 'all_items_SA', 'food_and_bev', 'food', 'presc_drugs', 'commodities', 'durables', 'nondurables', 'recreation', 'appliances', 'toys', 'apparel'])\n",
    "all_cpi['Year'] = all_items_NSA['Year']\n",
    "all_cpi['Month'] = all_items_NSA['Month']\n",
    "all_cpi['all_items_NSA'] = all_items_NSA['Value']\n",
    "all_cpi['all_items_SA'] = all_items_SA['Value']\n",
    "all_cpi['food_and_bev'] = food_and_bev['Value']\n",
    "all_cpi['food'] = food['Value']\n",
    "all_cpi['presc_drugs'] = presc_drugs['Value']\n",
    "all_cpi['commodities'] = commodities['Value']\n",
    "all_cpi['durables'] = durables['Value']\n",
    "all_cpi['nondurables'] = nondurables['Value']\n",
    "all_cpi['recreation'] = rec['Value']\n",
    "all_cpi['appliances'] = appliances['Value']\n",
    "all_cpi['toys'] = toys['Value']\n",
    "all_cpi['apparel'] = apparel['Value']\n",
    "\n",
    "all_cpi.head()\n",
    "\n",
    "all_cpi.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thoughtful discussion of ethical concerns included\n",
    "- Ethical concerns consider the whole data science process (question asked, data collected, data being used, the bias in data, analysis, post-analysis, etc.)\n",
    "- How your group handled bias/ethical concerns clearly described\n",
    "\n",
    "Acknowledge and address any ethics & privacy related issues of your question(s), proposed dataset(s), and/or analyses. Use the information provided in lecture to guide your group discussion and thinking. If you need further guidance, check out [Deon's Ethics Checklist](http://deon.drivendata.org/#data-science-ethics-checklist). In particular:\n",
    "\n",
    "- Are there any biases/privacy/terms of use issues with the data you propsed?\n",
    "- Are there potential biases in your dataset(s), in terms of who it composes, and how it was collected, that may be problematic in terms of it allowing for equitable analysis? (For example, does your data exclude particular populations, or is it likely to reflect particular human biases in a way that could be a problem?)\n",
    "- How will you set out to detect these specific biases before, during, and after/when communicating your analysis?\n",
    "- Are there any other issues related to your topic area, data, and/or analyses that are potentially problematic in terms of data privacy and equitable impact?\n",
    "- How will you handle issues you identified?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read over the [COGS108 Team Policies](https://github.com/COGS108/Projects/blob/master/COGS108_TeamPolicies.md) individually. Then, include your group’s expectations of one another for successful completion of your COGS108 project below. Discuss and agree on what all of your expectations are. Discuss how your team will communicate throughout the quarter and consider how you will communicate respectfully should conflicts arise. By including each member’s name above and by adding their name to the submission, you are indicating that you have read the COGS108 Team Policies, accept your team’s expectations below, and have every intention to fulfill them. These expectations are for your team’s use and benefit — they won’t be graded for their details.\n",
    "\n",
    "* *Team Expectation 1*\n",
    "* *Team Expectation 2*\n",
    "* *Team Expecation 3*\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your team's specific project timeline. An example timeline has been provided. Changes the dates, times, names, and details to fit your group's plan.\n",
    "\n",
    "If you think you will need any special resources or training outside what we have covered in COGS 108 to solve your problem, then your proposal should state these clearly. For example, if you have selected a problem that involves implementing multiple neural networks, please state this so we can make sure you know what you’re doing and so we can point you to resources you will need to implement your project. Note that you are not required to use outside methods.\n",
    "\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM | Read & Think about COGS 108 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data (Ant Man); EDA (Hulk) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin Analysis (Iron Man; Thor) | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project |\n",
    "| 3/20  | Before 11:59 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
