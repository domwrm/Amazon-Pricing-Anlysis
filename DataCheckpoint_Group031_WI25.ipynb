{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you lost points on the last checkpoint you can get them back by responding to TA/IA feedback**  \n",
    "\n",
    "Update/change the relevant sections where you lost those points, make sure you respond on GitHub Issues to your TA/IA to call their attention to the changes you made here.\n",
    "\n",
    "Please update your Timeline... no battle plan survives contact with the enemy, so make sure we understand how your plans have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Ryan Lindberg\n",
    "- Nathan Mitchell\n",
    "- Domonick Marshall\n",
    "- Sean Notolli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"How have Amazon product prices across different market sectors changed between 2021-2024, and how do these trends compare to equivalent categories in the Consumer Price Index (CPI)? Is Amazon pricing in line with overall inflation, or does it diverge from broader economic trends?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Amazon has been a staple ecommerce service in many lives across the globe, bested by no other. Due to its broad market and utmost convenience, Amazon is one of the first markets considered when needing anything. However, one might wonder how costly this convenience is.\n",
    "\n",
    "From the Amazon Product Pricing Report 2024 on Issuu <a name=\"cite_ref-1\"></a>[<sup>1</sup>](#cite_note-1), we can see that Amazon prices are influenced by a vast amount of factors such as supply and demand, seasonal trends, competition, shifting seller fees, and algorithmic pricing. Other elements such as brand power, customer reviews, and holiday shopping behavior also contribute to pricing variability. This report serves as essential information for this project as it provides context to better interpret and analyze Amazon price trends. The report provides information on different pricing trends across different sectors of the market such as beauty, home and kitchen, arts and crafts, pet supply, and baby products, and demonstrates how each sector faces different trends. With each market sector, the report also produces concise and clear visualizations of pricing data across multiple Amazon products. \n",
    "\n",
    "The Consumer Price Index (CPI) Summary from the US Bureau of Labor Statistics <a name=\"cite_ref-2\"></a>[<sup>2</sup>](#cite_note-2) offers guidance as to how to structure and analyze data related to our topic. Their report on CPI changes from 2023-2024 exemplifies how to organize large datasets and distill them into clear, actionable insights. The summary’s consistent formatting and emphasis on year-over-year percentage changes allow for a straightforward understanding of trends in consumer prices across different sectors. The structured approach will be instrumental in our own analysis of pricing data, helping us standardize our methodology and avoid potential misinterpretations. By adopting their organizational strategy, we can enhance the accuracy and credibility of our findings.\n",
    "\n",
    "Recent research has also talked about the idea of practical implications on dynamic algorithms in pricing on market behavior. Elmaghraby and Keskinocak <a name=\"cite_ref-3\"></a>[<sup>3</sup>](#cite_note-3) provide a review of dynamic pricing models explaining that factors like consumer demand and supply constraints drive pricing decisions in various industries. The paper talks about how algorithmic pricing is not only a tool for optimizing revenue but it can contribute to pricing volatility and competitive differences in the digital market. Using their findings with the data from Amazon product pricing report and CPI lets us understand how algorithmic strategies and external market conditions can interact with the price trends, Underscoring how important advanced computational methods are in predicting market behaviors in a retail environment.\n",
    "\n",
    "1. <a name=\"cite_note-1\"></a> [^](#cite_ref-1) Jungle Scout (2024, Jan). Amazon Product Pricing Report 2024. Issuu. https://issuu.com/junglescoutcobalt/docs/jungle-scout-amazon-product-pricing-report-2024?utm_source=chatgpt.html \n",
    "2. <a name=\"cite_note-2\"></a> [^](#cite_ref-2) Bureau of Labor Statistics (2024, Feb). Consumer Price Index Summary. U.S. Department of Labor. https://www.bls.gov/news.release/pdf/cpi.html\n",
    "3. <a name=\"cite_note-3\"></a> [^](#cite_ref-3) Elmaghraby, W., & Keskinocak, P. (2003). Dynamic Pricing in the Presence of Inventory Considerations: Research Overview, Current Practices, and Future Directions. Management Science, 49(10), https://www.researchgate.net/publication/220534328_Dynamic_Pricing_in_the_Presence_of_Inventory_Considerations_Research_Overview_Current_Practices_and_Future_Directions.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Amazon prices across different market sectors have generally increased from 2021-2024 but at an inconsistent rate with inflation trends in the consumer price index. We predict that discretionary goods like electronics and other luxury items have had smaller price increases when compared to essential goods like groceries and other necessities potentially exceeding the CPI inflation rate. We think this is due to Amazon using aggressive price matching and algorithms to remain competitive in non-essential categories where supply chain constraints and labor costs disproportionately impacted essential goods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data overview\n",
    "\n",
    "- Dataset #1\n",
    "  - Dataset Name: Keepa\n",
    "  - Link to the dataset: https://keepa.com/#!data\n",
    "  - Number of observations: 998\n",
    "  - Number of variables: 63\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - Dataset Name: Consumer Price Index (CPI)\n",
    "  - Link to the dataset: https://www.bls.gov/cpi/tables/supplemental-files/ \n",
    "  - Number of observations: 588\n",
    "  - Number of variables: 4\n",
    "\n",
    "Keepa is a database that stores detailed information on Amazon products. By leveraging their online API, we can retrieve data segmented by price, time, product ID, and various other attributes. Additionally, we have obtained CPI data from the U.S. Bureau of Labor Statistics, allowing us to analyze average prices across different product categories over specific time periods. After cleaning and wrangling both datasets, we will be able to compare them to draw meaningful conclusions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the keepa library (run this cell if not already installed)\n",
    "#!pip install keepa\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import keepa\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Price History Dataset from Keepa API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://keepa.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY = \"df2mtauj1tmrngcm95ubshd41fplpf2bfh1nba8s8hpd2m6golbbrj9bat7osb8o\" # do no share outside of private repo!!\n",
    "api = keepa.Keepa(ACCESS_KEY, timeout=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the price of a given amazon product, there are many different types of the the 'price' variable we can access. Two of them are new price and listing price. Here are the differences:\n",
    "\n",
    "• NEW PRICE:\n",
    "This is the current selling price for an item offered on Amazon in brand new condition. It reflects the actual market price that customers pay—typically the lowest available offer among Amazon and third‑party sellers. Because it’s influenced by promotions, competition, and real‑time market conditions, the NEW price can fluctuate over time.\n",
    "\n",
    "• LISTING PRICE:\n",
    "This is usually the manufacturer’s suggested retail price (MSRP) or the original price displayed on the product’s listing. It tends to be more stable and is often used as a reference to show discounts or price reductions. Even when the new price drops (for deals or competitive reasons), the listing price may remain unchanged.\n",
    "\n",
    "The new price shows you what you’d pay right now, while the listing price is a reference value set by the manufacturer. This difference helps sellers and buyers gauge the discount depth and market dynamics.\n",
    "\n",
    "Since we want to accurately capture the price a consumer is paying at a given time, we will use the NEW PRICE.\n",
    "\n",
    "Other price variables we can access are the NEW_FBM (Filled by Manufacturer), NEW_FBA (Filled by Amazon), USED, (used items), REFURBISHED (refurbished items), WAREHOUSE (prices from amazon warhouse deals, usually returned items). These price variables offer spotty data coverage, and don't accurately represent the consumer experience, so we will not be using them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to gather data for a variety of Amazon products from different sectors of the market. Luckily, each product on Amazon has a unique ASIN (Amazon Standard Identification Number) which we can use to identify it. With help from the Keepa Data Product Finder tool, we can collect these ASINs along with other product information. In the following example we use the Product Finder to filter by 'product category' = grocery_and_foods, and rank = 1-1000. This finds the top 1000 items categorized under 'grocery and foods' on Amazon. Additionally we can collect the name of the product, and the sub category. We made sure to exclude 'variations' of products, because we dont want to sample the same product 5 times just because there are 5 versions of the product with slight variations. We also refined our query to only physical products, excluding digital products and ebooks. CPI data is only collected on physical goods, so this allows us to have a fairer comparison. And our final filter is 'tracking time' which allows us to only select products that have been tracked by Keepa from 2021-2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in top 1000 baby products csv:\n",
    "data_path = Path('data/keepa_data') / 'grocery_and_foods.csv'\n",
    "products = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Categories: Sub</th>\n",
       "      <th>ASIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CELSIUS Sparkling Strawberry Guava, Functional...</td>\n",
       "      <td>Energy Drinks, Gluten-Free Groceries, Evergree...</td>\n",
       "      <td>B08PGXDHTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nespresso Capsules Vertuo, Voltesso, Mild Roas...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Kitchen &amp; Dining...</td>\n",
       "      <td>B0768N9N6P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premier Protein Shake, Cookies &amp; Cream, 30g Pr...</td>\n",
       "      <td>Protein Drinks, Beverages, Protein drinks</td>\n",
       "      <td>B07MFYYZ5B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparkling Ice, Peach Nectarine Sparkling Water...</td>\n",
       "      <td>Carbonated Water, Subscribe &amp; Save Prime Promo...</td>\n",
       "      <td>B009S2XFVW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sparkling Ice, Cranberry Frost Sparkling Water...</td>\n",
       "      <td>Soft Drinks, Sparkling water, Sparkling Water,...</td>\n",
       "      <td>B07KY58NFX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monster Energy Zero Ultra, Sugar Free Energy D...</td>\n",
       "      <td>Energy Drinks, Subscribe &amp; Save Prime Promo, B...</td>\n",
       "      <td>B00ADYXY7E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Core Power Protein Shake, Chocolate, 26g Bottl...</td>\n",
       "      <td>Protein Drinks, Balanced Nutrition, Prime Memb...</td>\n",
       "      <td>B07LD2NV9X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Starbucks K-Cup Coffee Pods, Medium Roast Coff...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Packaged Coffee,...</td>\n",
       "      <td>B00U3ODTTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nespresso Capsules Vertuo, Double Espresso Scu...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Kitchen &amp; Dining...</td>\n",
       "      <td>B07M8YV12G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nespresso Capsules VertuoLine, Hazelino Muffin...</td>\n",
       "      <td>Single-Serve Capsules &amp; Pods, Packaged Coffee,...</td>\n",
       "      <td>B0851ZVCGL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  CELSIUS Sparkling Strawberry Guava, Functional...   \n",
       "1  Nespresso Capsules Vertuo, Voltesso, Mild Roas...   \n",
       "2  Premier Protein Shake, Cookies & Cream, 30g Pr...   \n",
       "3  Sparkling Ice, Peach Nectarine Sparkling Water...   \n",
       "4  Sparkling Ice, Cranberry Frost Sparkling Water...   \n",
       "5  Monster Energy Zero Ultra, Sugar Free Energy D...   \n",
       "6  Core Power Protein Shake, Chocolate, 26g Bottl...   \n",
       "7  Starbucks K-Cup Coffee Pods, Medium Roast Coff...   \n",
       "8  Nespresso Capsules Vertuo, Double Espresso Scu...   \n",
       "9  Nespresso Capsules VertuoLine, Hazelino Muffin...   \n",
       "\n",
       "                                     Categories: Sub        ASIN  \n",
       "0  Energy Drinks, Gluten-Free Groceries, Evergree...  B08PGXDHTC  \n",
       "1  Single-Serve Capsules & Pods, Kitchen & Dining...  B0768N9N6P  \n",
       "2          Protein Drinks, Beverages, Protein drinks  B07MFYYZ5B  \n",
       "3  Carbonated Water, Subscribe & Save Prime Promo...  B009S2XFVW  \n",
       "4  Soft Drinks, Sparkling water, Sparkling Water,...  B07KY58NFX  \n",
       "5  Energy Drinks, Subscribe & Save Prime Promo, B...  B00ADYXY7E  \n",
       "6  Protein Drinks, Balanced Nutrition, Prime Memb...  B07LD2NV9X  \n",
       "7  Single-Serve Capsules & Pods, Packaged Coffee,...  B00U3ODTTM  \n",
       "8  Single-Serve Capsules & Pods, Kitchen & Dining...  B07M8YV12G  \n",
       "9  Single-Serve Capsules & Pods, Packaged Coffee,...  B0851ZVCGL  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the ASINs, product name, and subcategory, we've noticed that many products have multiple sub categories, most of which are irrelevant for the purposes of our analysis such as \"Christmas Store\", \"TEST ABCDEFGPD\" and even random characters such as \"d963aedb-8e7e-493c...\". We want to remove clean the subcategory column so that each product has a single subcategory. Luckily for us, it seems that the first sub category is the most descriptive subcategory of a given product, so we can remove the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categories: Sub\n",
       "White, Pantry, Step 3. Build your base, Gluten-Free Groceries, Plant-Based Lifestyles                                                                                                                                                                                                                                                                                                                                                                                        1\n",
       "Gummy Candy, TEST ABCDEFGPD, Save 25% on Easter Candy and Gum, Back to School: Cookies & Treats, Candy & Chocolate, Featured SNAP-eligible groceries, Grocery Easter, Canned, Packaged & Baking, All Candy Coupons, Movie Night Coupons, Non-chocolate candy                                                                                                                                                                                                                 1\n",
       "Peppercorns, Condiments & sauces, Pantry Staples                                                                                                                                                                                                                                                                                                                                                                                                                             1\n",
       "Snack & Trail Mixes, Snacks & produce IA, Paleo-friendly foods, Game day snacks & dips, Snacks, Snack favorites, Salty snacks, Snacks Under 5$, 15% off coupon terms and conditions, Snacks, Featured SNAP-eligible groceries, Snacks, Snacks, Plant-Based Lifestyles, Game Day Snacks, Canned, Packaged & Baking, OTC, Snacks under $5, Chips and Snacks, New to Fresh Snacks, Snacks, RFS Food, Other cheese pairings, Nuts & Trail Mix, Shop & Save Terms & Conditions    1\n",
       "Gummy Candy, Candy & Chocolate, Gluten-Free Groceries, SNAP_Sweets, Canned, Packaged & Baking, All Candy Coupons, Easter Selection, National Candy Month, Ready for School, Snacks, Condiments, Baking, $1, Gummy candy, Movie Night Coupons, DCA2 - Sweet Snacks, Non-chocolate candy                                                                                                                                                                                       1\n",
       "Sports Drinks, Refreshing beverages, Beverages, Quench your thirst, Beverages, Beverages, Memorial Day Coupons, Sports drinks, OTC, Sports Drinks, Save up to 20% on Select Gatorade Products, Ready for School, Fresh Grocery Coupons, Drinks - Sandwich Store, Ad-Leftt-Bulk- Beverage, Beverages, RFS Food, Kickoff to summer, Springtime Beverages, Beverages for Summer, Drinks                                                                                         1\n",
       "Meat & Seafood, Meat & Seafood, NYNY Snacks, APS Test, Meat & seafood, Featured SNAP-eligible groceries, SNAP Snacks, Fresh, Quick Meals & Snacks, Canned, Packaged & Baking, Memorial Day Coupons, OTC, Eat In ASINS, Jack Links, RFS Food, Snacks, Meat & Seafood                                                                                                                                                                                                          1\n",
       "Tuna Fish, Pantry, Pack in protein, Canned Tuna & Seafood, ILM ASINs A, Pantry Staples, Specialty Diets_keto (total), Deli Meats & Tuna - Sandwich Store, Pantry staples, SANDWICH STORE, Keto                                                                                                                                                                                                                                                                               1\n",
       "Single-Serve Capsules & Pods, Packaged Coffee, Cozy up with coffee, Coffee & Tea, Coffee, Save on Coffee & Tea, 1, Beverages, OTC, Coffee & creamer, Main Coffee Base Shoveler, Coffee & Tea, Cups/Pods                                                                                                                                                                                                                                                                      1\n",
       "Dried Dates, Salad toppings, Snacks & produce IA, Game day snacks & dips, Snacks, APS Test, Safe & Healthy Customer Favorites, Snacks Under 5$, 15% off coupon terms and conditions, Salads, Fresh produce, Snacks, Nuts, Seeds, Dried Fruits & Vegetables, Fresh, Game Day Snacks, Canned, Packaged & Baking, OTC, Snacks under $5, Fresh and dried fruit, Deals_FreshProduce, New to Fresh Snacks, RFS Food, Fruit Snacks & Dried Fruit, Shop & Save Terms & Conditions    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['Categories: Sub'].value_counts().tail(10) # bottom 10 subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function remove extra subcategories from each row and rename columns\n",
    "def clean_frame(df):\n",
    "    df = df.rename(columns = {'Title' : 'product_title', 'Categories: Sub' : 'subcategory', 'ASIN' : 'asin'})\n",
    "    def clean_row(row):\n",
    "        row['subcategory'] = row['subcategory'].split(',')[0].strip().lower()\n",
    "        return row\n",
    "    df = df.apply(clean_row, axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "products = clean_frame(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subcategory\n",
       "baking powder    1\n",
       "canola           1\n",
       "bitters          1\n",
       "frosting         1\n",
       "chile paste      1\n",
       "mixed            1\n",
       "brown sugar      1\n",
       "ground pepper    1\n",
       "fruit            1\n",
       "crackers         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['subcategory'].value_counts().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['subcategory'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, we can use the ASINs column to query the Keepa API for historical price data of each item.\n",
    "We define multiple functions below to achieve this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. keepa_time_to_datetime(kt):**\n",
    "\n",
    "This function takes in a keepa time integer, and converts it to standard time. Each price value has an attached time value so we know when that price was in effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. def generate_monthly_headers(days):**\n",
    "\n",
    "This function allows us to standardize which months we actually want to collect data for. When we query Keepa, it may return data from 2011 if the data is available. We dont want that data. So we can use this function to ensure that we filter the data for the months we need. It is also helpful in ensuring every time we collect data within a given range, the column headers are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. def get_monthly_avg_prices(asins, days):**\n",
    "\n",
    "This function gathers historical price data for a list of Amazon products using their ASINs. It returns a dataframe of the average price from the previous x amount of days for each product. We want data since 2020, so we will be using (365 * 5) for our days variable. It has some special features such as forward filling, which is helpful to fill in missing data. Since Keepa only updates prices when the price changes, and some months don't have any price changes, the natural solution is to forward fill to fill in missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our basic membership access to the Keepa API, we are limited with how much data we can request, so we will have to produce our price data in batches, and incrementally build up a large csv of price data for different product categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. def batch(iterable, n=20):**\n",
    "\n",
    "This takes in a list of ASINs, and returns batches of them of size n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**5. def query_keepa_in_batches(products, category, max_batches=10, batch_size=20, days=365 * 4, start_index=0, stop_index=None):**\n",
    "\n",
    "To put it simply, this function just gets monthly avg price data for a batch of ASINs, then merges it to a master csv.\n",
    "\n",
    "With the function query_keepa_in_batches, we can incrementally build a csv of historical price data for a given category by repeatedly requesting data from the Keepa API. The inputs to this function are: a dataframe such as grocery_and_foods, the category we are working with, i.e. the string 'grocery_and_foods', the number of batches we want, batch size, how many days of historical price data we want, and the start and stop indices of the ASINs we want to query from the dataframe. These start and stop indices allow us to pick up where we left off. So if for some reason the query stops, we can just figure out the last batch we completed, and keep going from there! If the CSV already exists, we just append to it. If it does not exist, we create a new one. This ensures we never overwrite data, and we can always keep track of where we are in the request process. This function takes many hours to run since we are limited to 1 token per minute with our API membership, and each ASIN costs 1 token. So it will have to run overnight to collect our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepa_time_to_datetime(kt):\n",
    "    # Convert Keepa time (minutes since 2011-01-01) to a Python datetime (UTC)\n",
    "    if isinstance(kt, datetime.datetime):\n",
    "        return kt\n",
    "    return datetime.datetime.fromtimestamp((kt + 21564000) * 60, datetime.timezone.utc)\n",
    "\n",
    "def generate_monthly_headers(days):\n",
    "    \"\"\"\n",
    "    Generate a list of month headers (strings) in the format 'YYYY-MM'\n",
    "    spanning from the current month back to the month that includes (now - days).\n",
    "    The headers are in ascending order. 2021-2025\n",
    "    This function standardizes which months we collect for each batch and ensures the columns are aligned.\n",
    "    \"\"\"\n",
    "    now = datetime.datetime.now(datetime.timezone.utc)\n",
    "    start_date = now - datetime.timedelta(days=days)\n",
    "    \n",
    "    headers = []\n",
    "    current_year = now.year\n",
    "    current_month = now.month\n",
    "\n",
    "    while True:\n",
    "        header = f\"{current_year:04d}-{current_month:02d}\"\n",
    "        headers.append(header)\n",
    "        # Move to the previous month\n",
    "        if current_month == 1:\n",
    "            current_month = 12\n",
    "            current_year -= 1\n",
    "        else:\n",
    "            current_month -= 1\n",
    "        \n",
    "        # Create a timezone-aware date for the first day of the new month.\n",
    "        month_start = datetime.datetime(current_year, current_month, 1, tzinfo=datetime.timezone.utc)\n",
    "        # Stop if this month is before the start_date.\n",
    "        if month_start < start_date:\n",
    "            break\n",
    "    return sorted(headers)\n",
    "\n",
    "\n",
    "def get_monthly_avg_prices(asins, days=1460):\n",
    "    \"\"\"\n",
    "    asins: list of ASIN strings\n",
    "    days: number of days of history (default 1460 ~ 4 years)\n",
    "    \n",
    "    Returns a DataFrame:\n",
    "        - Rows = ASINs\n",
    "        - Columns = monthly time periods (e.g. '2025-02', '2025-01', etc.)\n",
    "        - Values = average 'NEW' price for that month\n",
    "    \"\"\"\n",
    "    products = api.query(asins, days=days)\n",
    "    dfs = []\n",
    "    for product in products:\n",
    "        asin = product['asin']\n",
    "        price_history = product['data'].get('NEW', [])\n",
    "        time_history  = product['data'].get('NEW_time', [])\n",
    "        \n",
    "        dates = [keepa_time_to_datetime(t) for t in time_history]\n",
    "        prices = [p for p in price_history]\n",
    "        \n",
    "        df = pd.DataFrame({'date': dates, asin: prices})\n",
    "        df.set_index('date', inplace=True)\n",
    "        \n",
    "        # Resample to monthly average using month-end frequency\n",
    "        monthly_avg = df.resample('ME').mean()\n",
    "        dfs.append(monthly_avg)\n",
    "    \n",
    "    if not dfs:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine and transpose so that rows = ASIN and columns = dates\n",
    "    combined = pd.concat(dfs, axis=1).T\n",
    "    # Convert datetime columns to string format 'YYYY-MM'\n",
    "    combined.columns = [col.strftime('%Y-%m') for col in combined.columns]\n",
    "    \n",
    "    # Generate the complete set of monthly headers (headers are in descending order)\n",
    "    headers = generate_monthly_headers(days)\n",
    "    \n",
    "    # Reindex with headers generated headers\n",
    "    combined = combined.reindex(columns=headers, fill_value=np.nan)\n",
    "    \n",
    "    # Forward fill missing values along the row (in chronological order)\n",
    "    combined = combined.ffill(axis=1)\n",
    "    \n",
    "    return combined\n",
    "\n",
    "\n",
    "def batch(iterable, n=20):\n",
    "    \"\"\"Yield successive n-sized chunks from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2021-03</th>\n",
       "      <th>2021-04</th>\n",
       "      <th>2021-05</th>\n",
       "      <th>2021-06</th>\n",
       "      <th>2021-07</th>\n",
       "      <th>2021-08</th>\n",
       "      <th>2021-09</th>\n",
       "      <th>2021-10</th>\n",
       "      <th>2021-11</th>\n",
       "      <th>2021-12</th>\n",
       "      <th>...</th>\n",
       "      <th>2024-05</th>\n",
       "      <th>2024-06</th>\n",
       "      <th>2024-07</th>\n",
       "      <th>2024-08</th>\n",
       "      <th>2024-09</th>\n",
       "      <th>2024-10</th>\n",
       "      <th>2024-11</th>\n",
       "      <th>2024-12</th>\n",
       "      <th>2025-01</th>\n",
       "      <th>2025-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B009S2XFVW</th>\n",
       "      <td>NaN</td>\n",
       "      <td>13.32358</td>\n",
       "      <td>14.412791</td>\n",
       "      <td>7.844828</td>\n",
       "      <td>7.844828</td>\n",
       "      <td>10.955</td>\n",
       "      <td>8.99</td>\n",
       "      <td>8.520645</td>\n",
       "      <td>8.169459</td>\n",
       "      <td>8.169459</td>\n",
       "      <td>...</td>\n",
       "      <td>11.99</td>\n",
       "      <td>11.99</td>\n",
       "      <td>11.7</td>\n",
       "      <td>14.995</td>\n",
       "      <td>13.163333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.976</td>\n",
       "      <td>14.976</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            2021-03   2021-04    2021-05   2021-06   2021-07  2021-08  \\\n",
       "B009S2XFVW      NaN  13.32358  14.412791  7.844828  7.844828   10.955   \n",
       "\n",
       "            2021-09   2021-10   2021-11   2021-12  ...  2024-05  2024-06  \\\n",
       "B009S2XFVW     8.99  8.520645  8.169459  8.169459  ...    11.99    11.99   \n",
       "\n",
       "            2024-07  2024-08    2024-09  2024-10  2024-11  2024-12  2025-01  \\\n",
       "B009S2XFVW     11.7   14.995  13.163333     15.0   14.976   14.976     12.0   \n",
       "\n",
       "            2025-02  \n",
       "B009S2XFVW     12.0  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXAMPLE USAGE of get_monthly_avg_prices\n",
    "asins = [\"B009S2XFVW\"]  \n",
    "monthly_prices = get_monthly_avg_prices(asins, days = 365 * 4)\n",
    "monthly_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_keepa_in_batches(products, category, max_batches=10, batch_size=20,\n",
    "                           days=365 * 4, start_index=0, stop_index=None):\n",
    "    \"\"\"\n",
    "    Query Keepa for monthly average prices in batches and incrementally save \n",
    "    the *merged* results (column-aligned) to a CSV file.\n",
    "    \"\"\"\n",
    "    # Slice the ASIN list based on start_index and stop_index.\n",
    "    asins = list(products['asin'])[start_index:stop_index]\n",
    "    csv_file = f'data/asin_prices/{category}_monthly_prices.csv'\n",
    "    for i, asin_batch in enumerate(batch(asins, batch_size)):\n",
    "        if i >= max_batches:\n",
    "            break\n",
    "        df_batch = get_monthly_avg_prices(asin_batch, days=days)\n",
    "        if df_batch.empty:\n",
    "            print(f\"Batch {i+1} returned no data; skipping.\")\n",
    "            continue\n",
    "        # If the CSV file exists, read it, merge columns, then write back\n",
    "        if os.path.exists(csv_file):\n",
    "            existing_df = pd.read_csv(csv_file, index_col=0)\n",
    "            \n",
    "            # Merge on row index (ASIN) and combine columns (outer join).\n",
    "            # combine_first() will fill missing entries in existing_df with df_batch values.\n",
    "            merged_df = existing_df.combine_first(df_batch)\n",
    "            \n",
    "            # Ensure columns are in the correct order (descending monthly headers).\n",
    "            # This step uses the same monthly headers function to reindex.\n",
    "            headers = generate_monthly_headers(days)\n",
    "            merged_df = merged_df.reindex(columns=headers, fill_value=np.nan)\n",
    "            \n",
    "            merged_df.to_csv(csv_file, index=True)\n",
    "        else:\n",
    "            # If no CSV yet, just write df_batch as the first chunk\n",
    "            df_batch.to_csv(csv_file, index=True)\n",
    "        \n",
    "        print(f\"Batch {i+1} processed and merged.\")\n",
    "\n",
    "\n",
    "# Example helper function to split the ASIN list into batches.\n",
    "def batch(iterable, n=20): # 20 is default batch size\n",
    "    \"\"\"Yield successive n-sized chunks from iterable\"\"\"\n",
    "    for i in range(0, len(iterable), n):\n",
    "        yield iterable[i:i + n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This entire process was just for the baby_products category. We will have to do this same process for every other product category we use. Luckily after defining all of the functions, it shouldn't be that hard. We only need to do a couple simple steps:\n",
    "1. Load in a category of Amazon products as a dataframe\n",
    "2. Clean the dataframe\n",
    "3. Query the API to build historical price data csv. May take several hours due to token limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual use: \n",
    "category = 'grocery_and_foods'\n",
    "data_path = Path('data/keepa_data') / f'{category}.csv'\n",
    "grocery_and_foods = pd.read_csv(data_path)\n",
    "grocery_and_foods = clean_frame(grocery_and_foods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 processed and merged.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]Waiting 1175 seconds for additional tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from server: NOT_ENOUGH_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting 2 seconds for additional tokens\n",
      "100%|██████████| 20/20 [19:49<00:00, 59.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 processed and merged.\n"
     ]
    }
   ],
   "source": [
    "# Actual use:\n",
    "# This code may run for multiple hours. 1 minute for each product queried. \n",
    "query_keepa_in_batches(grocery_and_foods, category, max_batches=5, batch_size=20, days = (365 * 5) + 60, start_index = 900, stop_index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "The code of the cells above are mostly generated by ChatGPT from the prompt \"How can I load the Keepa API into an ipynb, and query price data every 30 days for the past 4 years?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining both dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>2020-01</th>\n",
       "      <th>2020-02</th>\n",
       "      <th>2020-03</th>\n",
       "      <th>2020-04</th>\n",
       "      <th>2020-05</th>\n",
       "      <th>2020-06</th>\n",
       "      <th>2020-07</th>\n",
       "      <th>2020-08</th>\n",
       "      <th>2020-09</th>\n",
       "      <th>...</th>\n",
       "      <th>2024-05</th>\n",
       "      <th>2024-06</th>\n",
       "      <th>2024-07</th>\n",
       "      <th>2024-08</th>\n",
       "      <th>2024-09</th>\n",
       "      <th>2024-10</th>\n",
       "      <th>2024-11</th>\n",
       "      <th>2024-12</th>\n",
       "      <th>2025-01</th>\n",
       "      <th>2025-02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000E5JIU</td>\n",
       "      <td>6.461333</td>\n",
       "      <td>6.522500</td>\n",
       "      <td>5.575000</td>\n",
       "      <td>5.575000</td>\n",
       "      <td>7.614286</td>\n",
       "      <td>7.365000</td>\n",
       "      <td>6.297500</td>\n",
       "      <td>5.572500</td>\n",
       "      <td>5.865833</td>\n",
       "      <td>...</td>\n",
       "      <td>9.386000</td>\n",
       "      <td>9.386000</td>\n",
       "      <td>9.485000</td>\n",
       "      <td>9.522727</td>\n",
       "      <td>9.011000</td>\n",
       "      <td>9.777143</td>\n",
       "      <td>9.732500</td>\n",
       "      <td>9.732500</td>\n",
       "      <td>9.817500</td>\n",
       "      <td>10.024286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0001UXQ9Q</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.232500</td>\n",
       "      <td>23.864118</td>\n",
       "      <td>11.935000</td>\n",
       "      <td>11.906154</td>\n",
       "      <td>11.318182</td>\n",
       "      <td>10.860000</td>\n",
       "      <td>10.650000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.285000</td>\n",
       "      <td>19.070714</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>12.800000</td>\n",
       "      <td>13.796667</td>\n",
       "      <td>18.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0002LD9IW</td>\n",
       "      <td>7.810000</td>\n",
       "      <td>5.578182</td>\n",
       "      <td>4.962500</td>\n",
       "      <td>4.032000</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>4.756667</td>\n",
       "      <td>3.818750</td>\n",
       "      <td>...</td>\n",
       "      <td>1.353333</td>\n",
       "      <td>1.353333</td>\n",
       "      <td>1.353333</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>4.035000</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.050000</td>\n",
       "      <td>4.032000</td>\n",
       "      <td>4.032000</td>\n",
       "      <td>4.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00032BPCM</td>\n",
       "      <td>3.847778</td>\n",
       "      <td>3.553333</td>\n",
       "      <td>7.652326</td>\n",
       "      <td>4.737206</td>\n",
       "      <td>1.565556</td>\n",
       "      <td>1.565556</td>\n",
       "      <td>1.565556</td>\n",
       "      <td>1.637500</td>\n",
       "      <td>3.287568</td>\n",
       "      <td>...</td>\n",
       "      <td>1.588947</td>\n",
       "      <td>2.105714</td>\n",
       "      <td>5.160645</td>\n",
       "      <td>4.680000</td>\n",
       "      <td>5.980000</td>\n",
       "      <td>4.951667</td>\n",
       "      <td>5.269259</td>\n",
       "      <td>5.179375</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>5.057308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000AXW9XI</td>\n",
       "      <td>10.208286</td>\n",
       "      <td>11.024000</td>\n",
       "      <td>11.477500</td>\n",
       "      <td>14.070645</td>\n",
       "      <td>12.530238</td>\n",
       "      <td>11.881081</td>\n",
       "      <td>11.717692</td>\n",
       "      <td>11.456486</td>\n",
       "      <td>10.265370</td>\n",
       "      <td>...</td>\n",
       "      <td>10.024706</td>\n",
       "      <td>10.461000</td>\n",
       "      <td>11.595000</td>\n",
       "      <td>17.990000</td>\n",
       "      <td>17.980556</td>\n",
       "      <td>17.970000</td>\n",
       "      <td>18.993333</td>\n",
       "      <td>19.990000</td>\n",
       "      <td>16.180000</td>\n",
       "      <td>14.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>B08QR1MWHF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.690000</td>\n",
       "      <td>22.180000</td>\n",
       "      <td>22.886000</td>\n",
       "      <td>19.423333</td>\n",
       "      <td>19.423333</td>\n",
       "      <td>18.924706</td>\n",
       "      <td>17.882000</td>\n",
       "      <td>18.787143</td>\n",
       "      <td>18.394444</td>\n",
       "      <td>19.733571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>B08QTTJ1NM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>24.990000</td>\n",
       "      <td>8.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>B08R4K3LR6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>41.980000</td>\n",
       "      <td>41.980000</td>\n",
       "      <td>46.381053</td>\n",
       "      <td>45.826667</td>\n",
       "      <td>43.594762</td>\n",
       "      <td>45.162727</td>\n",
       "      <td>45.162727</td>\n",
       "      <td>45.162727</td>\n",
       "      <td>44.874444</td>\n",
       "      <td>46.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>B08R6DZXYV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>13.864000</td>\n",
       "      <td>13.864000</td>\n",
       "      <td>13.036667</td>\n",
       "      <td>13.270000</td>\n",
       "      <td>14.405000</td>\n",
       "      <td>13.130000</td>\n",
       "      <td>12.920000</td>\n",
       "      <td>12.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>B08RD41V4Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.610000</td>\n",
       "      <td>15.305000</td>\n",
       "      <td>13.984444</td>\n",
       "      <td>13.984444</td>\n",
       "      <td>12.492500</td>\n",
       "      <td>19.950000</td>\n",
       "      <td>13.985000</td>\n",
       "      <td>13.985000</td>\n",
       "      <td>14.990000</td>\n",
       "      <td>14.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>998 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    2020-01    2020-02    2020-03    2020-04    2020-05  \\\n",
       "0    B0000E5JIU   6.461333   6.522500   5.575000   5.575000   7.614286   \n",
       "1    B0001UXQ9Q        NaN        NaN  22.232500  23.864118  11.935000   \n",
       "2    B0002LD9IW   7.810000   5.578182   4.962500   4.032000   4.756667   \n",
       "3    B00032BPCM   3.847778   3.553333   7.652326   4.737206   1.565556   \n",
       "4    B000AXW9XI  10.208286  11.024000  11.477500  14.070645  12.530238   \n",
       "..          ...        ...        ...        ...        ...        ...   \n",
       "993  B08QR1MWHF        NaN        NaN        NaN        NaN        NaN   \n",
       "994  B08QTTJ1NM        NaN        NaN        NaN        NaN        NaN   \n",
       "995  B08R4K3LR6        NaN        NaN        NaN        NaN        NaN   \n",
       "996  B08R6DZXYV        NaN        NaN        NaN        NaN        NaN   \n",
       "997  B08RD41V4Y        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "       2020-06    2020-07    2020-08    2020-09  ...    2024-05    2024-06  \\\n",
       "0     7.365000   6.297500   5.572500   5.865833  ...   9.386000   9.386000   \n",
       "1    11.906154  11.318182  10.860000  10.650000  ...  16.285000  19.070714   \n",
       "2     4.756667   4.756667   4.756667   3.818750  ...   1.353333   1.353333   \n",
       "3     1.565556   1.565556   1.637500   3.287568  ...   1.588947   2.105714   \n",
       "4    11.881081  11.717692  11.456486  10.265370  ...  10.024706  10.461000   \n",
       "..         ...        ...        ...        ...  ...        ...        ...   \n",
       "993        NaN        NaN        NaN        NaN  ...  20.690000  22.180000   \n",
       "994        NaN        NaN        NaN        NaN  ...  24.990000  24.990000   \n",
       "995        NaN        NaN        NaN        NaN  ...  41.980000  41.980000   \n",
       "996        NaN        NaN        NaN        NaN  ...  13.270000  13.270000   \n",
       "997        NaN        NaN        NaN        NaN  ...  15.610000  15.305000   \n",
       "\n",
       "       2024-07    2024-08    2024-09    2024-10    2024-11    2024-12  \\\n",
       "0     9.485000   9.522727   9.011000   9.777143   9.732500   9.732500   \n",
       "1    11.625000  11.625000  11.625000  11.625000  12.800000  12.800000   \n",
       "2     1.353333   4.035000   4.035000   4.050000   4.050000   4.032000   \n",
       "3     5.160645   4.680000   5.980000   4.951667   5.269259   5.179375   \n",
       "4    11.595000  17.990000  17.980556  17.970000  18.993333  19.990000   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "993  22.886000  19.423333  19.423333  18.924706  17.882000  18.787143   \n",
       "994  24.990000  24.990000   8.990000  14.990000  14.990000  14.990000   \n",
       "995  46.381053  45.826667  43.594762  45.162727  45.162727  45.162727   \n",
       "996  13.864000  13.864000  13.036667  13.270000  14.405000  13.130000   \n",
       "997  13.984444  13.984444  12.492500  19.950000  13.985000  13.985000   \n",
       "\n",
       "       2025-01    2025-02  \n",
       "0     9.817500  10.024286  \n",
       "1    13.796667  18.970000  \n",
       "2     4.032000   4.032000  \n",
       "3     5.166667   5.057308  \n",
       "4    16.180000  14.240000  \n",
       "..         ...        ...  \n",
       "993  18.394444  19.733571  \n",
       "994  14.990000  14.990000  \n",
       "995  44.874444  46.166667  \n",
       "996  12.920000  12.570000  \n",
       "997  14.990000  14.990000  \n",
       "\n",
       "[998 rows x 63 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'grocery_and_foods'\n",
    "data_path2 = Path('data/') / f'{category}_monthly_prices.csv'\n",
    "price_data = pd.read_csv(data_path2)\n",
    "\n",
    "price_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to notice is that rows near the end of the data tend to have less data in the earlier months. We suspect this is due to how Keepa sorts their ASINs. This shouldnt be too big of an issue though. With an extra years worth of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the dataframe by rounding values to 2 decimal places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prices(df):\n",
    "    # Rename 'Unnamed: 0' to 'asin'\n",
    "    df = df.rename(columns={'Unnamed: 0': 'asin'})\n",
    "    \n",
    "    # Round every value in all columns except the first (asin) to 2 decimal places\n",
    "    df.iloc[:, 1:] = df.iloc[:, 1:].round(2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = clean_prices(price_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unnecessary columns that are out of our project scope 2021-2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = list(price_data.columns[1:13]) + list(price_data.columns[-2:])\n",
    "price_data = price_data.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging with other dataframe to get product title and subcategory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(products, price_data, on='asin', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping rows with no price data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_and_foods = merged_data.dropna(subset=merged_data.columns[4:], how='all').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_title</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>asin</th>\n",
       "      <th>2022-01</th>\n",
       "      <th>2022-02</th>\n",
       "      <th>2022-03</th>\n",
       "      <th>2022-04</th>\n",
       "      <th>2022-05</th>\n",
       "      <th>2022-06</th>\n",
       "      <th>2022-07</th>\n",
       "      <th>...</th>\n",
       "      <th>2024-01</th>\n",
       "      <th>2024-02</th>\n",
       "      <th>2024-03</th>\n",
       "      <th>2024-04</th>\n",
       "      <th>2024-05</th>\n",
       "      <th>2024-06</th>\n",
       "      <th>2024-07</th>\n",
       "      <th>2024-08</th>\n",
       "      <th>2024-09</th>\n",
       "      <th>2024-10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CELSIUS Sparkling Strawberry Guava, Functional...</td>\n",
       "      <td>energy drinks</td>\n",
       "      <td>B08PGXDHTC</td>\n",
       "      <td>21.97</td>\n",
       "      <td>21.52</td>\n",
       "      <td>21.60</td>\n",
       "      <td>20.83</td>\n",
       "      <td>17.74</td>\n",
       "      <td>24.54</td>\n",
       "      <td>30.37</td>\n",
       "      <td>...</td>\n",
       "      <td>26.85</td>\n",
       "      <td>26.02</td>\n",
       "      <td>27.15</td>\n",
       "      <td>26.64</td>\n",
       "      <td>26.87</td>\n",
       "      <td>27.14</td>\n",
       "      <td>18.69</td>\n",
       "      <td>20.73</td>\n",
       "      <td>18.41</td>\n",
       "      <td>18.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nespresso Capsules Vertuo, Voltesso, Mild Roas...</td>\n",
       "      <td>single-serve capsules &amp; pods</td>\n",
       "      <td>B0768N9N6P</td>\n",
       "      <td>42.04</td>\n",
       "      <td>47.50</td>\n",
       "      <td>47.50</td>\n",
       "      <td>47.50</td>\n",
       "      <td>47.50</td>\n",
       "      <td>47.50</td>\n",
       "      <td>47.50</td>\n",
       "      <td>...</td>\n",
       "      <td>48.33</td>\n",
       "      <td>48.24</td>\n",
       "      <td>48.24</td>\n",
       "      <td>48.24</td>\n",
       "      <td>48.24</td>\n",
       "      <td>49.02</td>\n",
       "      <td>36.44</td>\n",
       "      <td>44.77</td>\n",
       "      <td>42.88</td>\n",
       "      <td>42.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Premier Protein Shake, Cookies &amp; Cream, 30g Pr...</td>\n",
       "      <td>protein drinks</td>\n",
       "      <td>B07MFYYZ5B</td>\n",
       "      <td>34.46</td>\n",
       "      <td>25.37</td>\n",
       "      <td>32.12</td>\n",
       "      <td>32.80</td>\n",
       "      <td>44.24</td>\n",
       "      <td>37.73</td>\n",
       "      <td>23.70</td>\n",
       "      <td>...</td>\n",
       "      <td>23.54</td>\n",
       "      <td>18.97</td>\n",
       "      <td>20.93</td>\n",
       "      <td>17.14</td>\n",
       "      <td>18.28</td>\n",
       "      <td>17.95</td>\n",
       "      <td>18.24</td>\n",
       "      <td>26.43</td>\n",
       "      <td>30.98</td>\n",
       "      <td>27.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparkling Ice, Peach Nectarine Sparkling Water...</td>\n",
       "      <td>carbonated water</td>\n",
       "      <td>B009S2XFVW</td>\n",
       "      <td>7.91</td>\n",
       "      <td>7.91</td>\n",
       "      <td>8.92</td>\n",
       "      <td>8.49</td>\n",
       "      <td>10.51</td>\n",
       "      <td>12.17</td>\n",
       "      <td>11.46</td>\n",
       "      <td>...</td>\n",
       "      <td>12.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>11.99</td>\n",
       "      <td>11.99</td>\n",
       "      <td>11.70</td>\n",
       "      <td>15.00</td>\n",
       "      <td>13.16</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monster Energy Zero Ultra, Sugar Free Energy D...</td>\n",
       "      <td>energy drinks</td>\n",
       "      <td>B00ADYXY7E</td>\n",
       "      <td>43.23</td>\n",
       "      <td>32.33</td>\n",
       "      <td>29.91</td>\n",
       "      <td>32.98</td>\n",
       "      <td>28.49</td>\n",
       "      <td>32.58</td>\n",
       "      <td>31.73</td>\n",
       "      <td>...</td>\n",
       "      <td>47.89</td>\n",
       "      <td>45.03</td>\n",
       "      <td>46.16</td>\n",
       "      <td>46.78</td>\n",
       "      <td>45.90</td>\n",
       "      <td>45.90</td>\n",
       "      <td>45.90</td>\n",
       "      <td>38.42</td>\n",
       "      <td>40.62</td>\n",
       "      <td>43.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>BetterBody Foods Organic Refined Coconut Oil, ...</td>\n",
       "      <td>coconut</td>\n",
       "      <td>B00U1RKGOW</td>\n",
       "      <td>14.33</td>\n",
       "      <td>15.13</td>\n",
       "      <td>16.89</td>\n",
       "      <td>15.63</td>\n",
       "      <td>15.59</td>\n",
       "      <td>15.59</td>\n",
       "      <td>15.59</td>\n",
       "      <td>...</td>\n",
       "      <td>13.90</td>\n",
       "      <td>16.80</td>\n",
       "      <td>16.52</td>\n",
       "      <td>17.41</td>\n",
       "      <td>17.07</td>\n",
       "      <td>17.67</td>\n",
       "      <td>17.76</td>\n",
       "      <td>18.93</td>\n",
       "      <td>19.00</td>\n",
       "      <td>17.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>Jack Link's Bacon Jerky, Hickory Smoked - Flav...</td>\n",
       "      <td>meat &amp; seafood</td>\n",
       "      <td>B01MR6UPAQ</td>\n",
       "      <td>4.14</td>\n",
       "      <td>4.25</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4.93</td>\n",
       "      <td>4.96</td>\n",
       "      <td>4.97</td>\n",
       "      <td>...</td>\n",
       "      <td>4.60</td>\n",
       "      <td>4.42</td>\n",
       "      <td>5.11</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.38</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.47</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.16</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>Kar’s Nuts Trail Mix Variety Pack, Pack of 24 ...</td>\n",
       "      <td>snack &amp; trail mixes</td>\n",
       "      <td>B00CPS6VTO</td>\n",
       "      <td>18.61</td>\n",
       "      <td>18.49</td>\n",
       "      <td>16.80</td>\n",
       "      <td>12.95</td>\n",
       "      <td>19.19</td>\n",
       "      <td>30.63</td>\n",
       "      <td>23.01</td>\n",
       "      <td>...</td>\n",
       "      <td>27.94</td>\n",
       "      <td>27.47</td>\n",
       "      <td>26.42</td>\n",
       "      <td>27.20</td>\n",
       "      <td>30.31</td>\n",
       "      <td>30.31</td>\n",
       "      <td>29.08</td>\n",
       "      <td>30.36</td>\n",
       "      <td>29.17</td>\n",
       "      <td>30.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>Traditional Medicinals Tea, Organic Echinacea ...</td>\n",
       "      <td>herbal</td>\n",
       "      <td>B01LTI9DM2</td>\n",
       "      <td>35.51</td>\n",
       "      <td>35.51</td>\n",
       "      <td>35.51</td>\n",
       "      <td>35.51</td>\n",
       "      <td>35.51</td>\n",
       "      <td>35.51</td>\n",
       "      <td>5.99</td>\n",
       "      <td>...</td>\n",
       "      <td>4.96</td>\n",
       "      <td>4.96</td>\n",
       "      <td>4.72</td>\n",
       "      <td>4.92</td>\n",
       "      <td>4.92</td>\n",
       "      <td>4.92</td>\n",
       "      <td>4.96</td>\n",
       "      <td>4.72</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>Jovial Organic Einkorn Unbleached All Purpose ...</td>\n",
       "      <td>wheat flours &amp; meals</td>\n",
       "      <td>B00OEQ8L8E</td>\n",
       "      <td>29.53</td>\n",
       "      <td>33.24</td>\n",
       "      <td>33.24</td>\n",
       "      <td>31.59</td>\n",
       "      <td>41.49</td>\n",
       "      <td>39.09</td>\n",
       "      <td>43.99</td>\n",
       "      <td>...</td>\n",
       "      <td>37.16</td>\n",
       "      <td>48.73</td>\n",
       "      <td>45.98</td>\n",
       "      <td>38.34</td>\n",
       "      <td>32.71</td>\n",
       "      <td>32.71</td>\n",
       "      <td>40.89</td>\n",
       "      <td>40.89</td>\n",
       "      <td>40.89</td>\n",
       "      <td>42.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>945 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         product_title  \\\n",
       "0    CELSIUS Sparkling Strawberry Guava, Functional...   \n",
       "1    Nespresso Capsules Vertuo, Voltesso, Mild Roas...   \n",
       "2    Premier Protein Shake, Cookies & Cream, 30g Pr...   \n",
       "3    Sparkling Ice, Peach Nectarine Sparkling Water...   \n",
       "4    Monster Energy Zero Ultra, Sugar Free Energy D...   \n",
       "..                                                 ...   \n",
       "940  BetterBody Foods Organic Refined Coconut Oil, ...   \n",
       "941  Jack Link's Bacon Jerky, Hickory Smoked - Flav...   \n",
       "942  Kar’s Nuts Trail Mix Variety Pack, Pack of 24 ...   \n",
       "943  Traditional Medicinals Tea, Organic Echinacea ...   \n",
       "944  Jovial Organic Einkorn Unbleached All Purpose ...   \n",
       "\n",
       "                      subcategory        asin  2022-01  2022-02  2022-03  \\\n",
       "0                   energy drinks  B08PGXDHTC    21.97    21.52    21.60   \n",
       "1    single-serve capsules & pods  B0768N9N6P    42.04    47.50    47.50   \n",
       "2                  protein drinks  B07MFYYZ5B    34.46    25.37    32.12   \n",
       "3                carbonated water  B009S2XFVW     7.91     7.91     8.92   \n",
       "4                   energy drinks  B00ADYXY7E    43.23    32.33    29.91   \n",
       "..                            ...         ...      ...      ...      ...   \n",
       "940                       coconut  B00U1RKGOW    14.33    15.13    16.89   \n",
       "941                meat & seafood  B01MR6UPAQ     4.14     4.25     4.95   \n",
       "942           snack & trail mixes  B00CPS6VTO    18.61    18.49    16.80   \n",
       "943                        herbal  B01LTI9DM2    35.51    35.51    35.51   \n",
       "944          wheat flours & meals  B00OEQ8L8E    29.53    33.24    33.24   \n",
       "\n",
       "     2022-04  2022-05  2022-06  2022-07  ...  2024-01  2024-02  2024-03  \\\n",
       "0      20.83    17.74    24.54    30.37  ...    26.85    26.02    27.15   \n",
       "1      47.50    47.50    47.50    47.50  ...    48.33    48.24    48.24   \n",
       "2      32.80    44.24    37.73    23.70  ...    23.54    18.97    20.93   \n",
       "3       8.49    10.51    12.17    11.46  ...    12.00    12.00    15.00   \n",
       "4      32.98    28.49    32.58    31.73  ...    47.89    45.03    46.16   \n",
       "..       ...      ...      ...      ...  ...      ...      ...      ...   \n",
       "940    15.63    15.59    15.59    15.59  ...    13.90    16.80    16.52   \n",
       "941     4.86     4.93     4.96     4.97  ...     4.60     4.42     5.11   \n",
       "942    12.95    19.19    30.63    23.01  ...    27.94    27.47    26.42   \n",
       "943    35.51    35.51    35.51     5.99  ...     4.96     4.96     4.72   \n",
       "944    31.59    41.49    39.09    43.99  ...    37.16    48.73    45.98   \n",
       "\n",
       "     2024-04  2024-05  2024-06  2024-07  2024-08  2024-09  2024-10  \n",
       "0      26.64    26.87    27.14    18.69    20.73    18.41    18.41  \n",
       "1      48.24    48.24    49.02    36.44    44.77    42.88    42.88  \n",
       "2      17.14    18.28    17.95    18.24    26.43    30.98    27.59  \n",
       "3      12.00    11.99    11.99    11.70    15.00    13.16    15.00  \n",
       "4      46.78    45.90    45.90    45.90    38.42    40.62    43.59  \n",
       "..       ...      ...      ...      ...      ...      ...      ...  \n",
       "940    17.41    17.07    17.67    17.76    18.93    19.00    17.20  \n",
       "941     5.00     4.38     3.47     3.47     3.35     3.16     5.00  \n",
       "942    27.20    30.31    30.31    29.08    30.36    29.17    30.63  \n",
       "943     4.92     4.92     4.92     4.96     4.72     4.54     4.85  \n",
       "944    38.34    32.71    32.71    40.89    40.89    40.89    42.96  \n",
       "\n",
       "[945 rows x 37 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grocery_and_foods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is clean, tidy, and ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset #2 Consumer Price Index (CPI) for All Urban Consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is a list of all separate data sets that we retreived from the CPI database.\n",
    "\n",
    "- [All items in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SA0)\n",
    "- [All items in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SA0)\n",
    "- [Food and beverages in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SAF)\n",
    "- [Food in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SAF1)\n",
    "- [Prescription drugs in U.S. city average, all urban consumers, not seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUUR0000SEMF01)\n",
    "- [Commodities in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAC)\n",
    "- [Durables in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAD)\n",
    "- [Nondurables in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAN)\n",
    "- [Recreation in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAR)\n",
    "- [Appliances in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SEHK)\n",
    "- [Toys in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SERE01)\n",
    "- [Apparel in U.S. city average, all urban consumers, seasonally adjusted](http://data.bls.gov/dataViewer/view/timeseries/CUSR0000SAA)\n",
    "\n",
    "Notes:\n",
    "- Seasonally adjusted indicates that data has been statistically modified (by the CPI) to remove the effects of predictable seasonal fluctuations, like holiday shopping sprees or summer vacation trends, allowing for a clearer analysis of underlying trends without the influence of recurring seasonal patterns.\n",
    "- Durables are essentially products that are designed to last a long time and be purhcased rather infrequently.\n",
    "Ex: Cars, refrigerators, furniture, washing machines and dryers, musical instruments, etc.\n",
    "- Nondurables are goods that are generally consumed quickly and purchased more frequently. They lose value after one use and/or after a short period of time. Ex: food, drinks, hygiene products, paper products, cosmetics, clothing items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>all_items_NSA</th>\n",
       "      <th>all_items_SA</th>\n",
       "      <th>food_and_bev</th>\n",
       "      <th>food</th>\n",
       "      <th>presc_drugs</th>\n",
       "      <th>commodities</th>\n",
       "      <th>durables</th>\n",
       "      <th>nondurables</th>\n",
       "      <th>recreation</th>\n",
       "      <th>appliances</th>\n",
       "      <th>toys</th>\n",
       "      <th>apparel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>Jan</td>\n",
       "      <td>261.582</td>\n",
       "      <td>262.639</td>\n",
       "      <td>270.260</td>\n",
       "      <td>270.938</td>\n",
       "      <td>525.434</td>\n",
       "      <td>189.420</td>\n",
       "      <td>107.972</td>\n",
       "      <td>230.924</td>\n",
       "      <td>122.170</td>\n",
       "      <td>81.268</td>\n",
       "      <td>28.768</td>\n",
       "      <td>119.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>Feb</td>\n",
       "      <td>263.014</td>\n",
       "      <td>263.573</td>\n",
       "      <td>270.672</td>\n",
       "      <td>271.363</td>\n",
       "      <td>521.231</td>\n",
       "      <td>190.400</td>\n",
       "      <td>108.229</td>\n",
       "      <td>232.507</td>\n",
       "      <td>122.759</td>\n",
       "      <td>82.603</td>\n",
       "      <td>28.783</td>\n",
       "      <td>118.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>Mar</td>\n",
       "      <td>264.877</td>\n",
       "      <td>264.847</td>\n",
       "      <td>271.135</td>\n",
       "      <td>271.812</td>\n",
       "      <td>521.375</td>\n",
       "      <td>191.908</td>\n",
       "      <td>108.754</td>\n",
       "      <td>234.155</td>\n",
       "      <td>123.192</td>\n",
       "      <td>83.148</td>\n",
       "      <td>28.944</td>\n",
       "      <td>118.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>Apr</td>\n",
       "      <td>267.054</td>\n",
       "      <td>266.625</td>\n",
       "      <td>272.367</td>\n",
       "      <td>273.090</td>\n",
       "      <td>523.876</td>\n",
       "      <td>193.647</td>\n",
       "      <td>111.951</td>\n",
       "      <td>234.973</td>\n",
       "      <td>124.261</td>\n",
       "      <td>83.029</td>\n",
       "      <td>29.669</td>\n",
       "      <td>119.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>May</td>\n",
       "      <td>269.195</td>\n",
       "      <td>268.404</td>\n",
       "      <td>273.441</td>\n",
       "      <td>274.212</td>\n",
       "      <td>522.116</td>\n",
       "      <td>195.923</td>\n",
       "      <td>114.749</td>\n",
       "      <td>235.969</td>\n",
       "      <td>124.605</td>\n",
       "      <td>83.705</td>\n",
       "      <td>29.402</td>\n",
       "      <td>120.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021</td>\n",
       "      <td>Jun</td>\n",
       "      <td>271.696</td>\n",
       "      <td>270.710</td>\n",
       "      <td>275.380</td>\n",
       "      <td>276.206</td>\n",
       "      <td>519.861</td>\n",
       "      <td>199.068</td>\n",
       "      <td>118.684</td>\n",
       "      <td>238.052</td>\n",
       "      <td>124.954</td>\n",
       "      <td>83.791</td>\n",
       "      <td>29.059</td>\n",
       "      <td>121.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021</td>\n",
       "      <td>Jul</td>\n",
       "      <td>273.003</td>\n",
       "      <td>271.965</td>\n",
       "      <td>277.187</td>\n",
       "      <td>278.127</td>\n",
       "      <td>519.957</td>\n",
       "      <td>200.178</td>\n",
       "      <td>119.531</td>\n",
       "      <td>239.489</td>\n",
       "      <td>125.633</td>\n",
       "      <td>84.065</td>\n",
       "      <td>29.111</td>\n",
       "      <td>121.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021</td>\n",
       "      <td>Aug</td>\n",
       "      <td>273.567</td>\n",
       "      <td>272.752</td>\n",
       "      <td>278.201</td>\n",
       "      <td>279.135</td>\n",
       "      <td>519.729</td>\n",
       "      <td>201.155</td>\n",
       "      <td>120.047</td>\n",
       "      <td>241.243</td>\n",
       "      <td>126.282</td>\n",
       "      <td>85.150</td>\n",
       "      <td>29.137</td>\n",
       "      <td>121.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021</td>\n",
       "      <td>Sep</td>\n",
       "      <td>274.310</td>\n",
       "      <td>273.942</td>\n",
       "      <td>280.452</td>\n",
       "      <td>281.506</td>\n",
       "      <td>521.506</td>\n",
       "      <td>202.684</td>\n",
       "      <td>120.120</td>\n",
       "      <td>243.527</td>\n",
       "      <td>126.521</td>\n",
       "      <td>86.230</td>\n",
       "      <td>28.775</td>\n",
       "      <td>121.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021</td>\n",
       "      <td>Oct</td>\n",
       "      <td>276.589</td>\n",
       "      <td>276.528</td>\n",
       "      <td>282.943</td>\n",
       "      <td>284.205</td>\n",
       "      <td>523.862</td>\n",
       "      <td>205.875</td>\n",
       "      <td>122.341</td>\n",
       "      <td>246.637</td>\n",
       "      <td>127.495</td>\n",
       "      <td>86.786</td>\n",
       "      <td>28.943</td>\n",
       "      <td>122.522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Month  all_items_NSA  all_items_SA  food_and_bev     food  \\\n",
       "0  2021   Jan        261.582       262.639       270.260  270.938   \n",
       "1  2021   Feb        263.014       263.573       270.672  271.363   \n",
       "2  2021   Mar        264.877       264.847       271.135  271.812   \n",
       "3  2021   Apr        267.054       266.625       272.367  273.090   \n",
       "4  2021   May        269.195       268.404       273.441  274.212   \n",
       "5  2021   Jun        271.696       270.710       275.380  276.206   \n",
       "6  2021   Jul        273.003       271.965       277.187  278.127   \n",
       "7  2021   Aug        273.567       272.752       278.201  279.135   \n",
       "8  2021   Sep        274.310       273.942       280.452  281.506   \n",
       "9  2021   Oct        276.589       276.528       282.943  284.205   \n",
       "\n",
       "   presc_drugs  commodities  durables  nondurables  recreation  appliances  \\\n",
       "0      525.434      189.420   107.972      230.924     122.170      81.268   \n",
       "1      521.231      190.400   108.229      232.507     122.759      82.603   \n",
       "2      521.375      191.908   108.754      234.155     123.192      83.148   \n",
       "3      523.876      193.647   111.951      234.973     124.261      83.029   \n",
       "4      522.116      195.923   114.749      235.969     124.605      83.705   \n",
       "5      519.861      199.068   118.684      238.052     124.954      83.791   \n",
       "6      519.957      200.178   119.531      239.489     125.633      84.065   \n",
       "7      519.729      201.155   120.047      241.243     126.282      85.150   \n",
       "8      521.506      202.684   120.120      243.527     126.521      86.230   \n",
       "9      523.862      205.875   122.341      246.637     127.495      86.786   \n",
       "\n",
       "     toys  apparel  \n",
       "0  28.768  119.233  \n",
       "1  28.783  118.398  \n",
       "2  28.944  118.481  \n",
       "3  29.669  119.495  \n",
       "4  29.402  120.612  \n",
       "5  29.059  121.171  \n",
       "6  29.111  121.318  \n",
       "7  29.137  121.569  \n",
       "8  28.775  121.124  \n",
       "9  28.943  122.522  "
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All items in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "all_items_NSA = pd.read_csv('data/cpi_data/all_items_NSA.csv')\n",
    "all_items_NSA = all_items_NSA.drop(columns=['Series ID'])\n",
    "all_items_NSA = all_items_NSA.drop(columns=['Period'])\n",
    "all_items_NSA['Month'] = all_items_NSA['Label'].apply(lambda x: x.split()[1])\n",
    "all_items_NSA = all_items_NSA.drop(columns=['Label'])\n",
    "all_items_NSA = all_items_NSA[['Year', 'Month', 'Value']]\n",
    "\n",
    "\n",
    "# All items in U.S. city average, all urban consumers, seasonally adjusted\n",
    "all_items_SA = pd.read_csv('data/cpi_data/all_items_SA.csv')\n",
    "all_items_SA = all_items_SA.drop(columns=['Series ID'])\n",
    "all_items_SA = all_items_SA.drop(columns=['Period'])\n",
    "all_items_SA['Month'] = all_items_SA['Label'].apply(lambda x: x.split()[1])\n",
    "all_items_SA = all_items_SA.drop(columns=['Label'])\n",
    "all_items_SA = all_items_SA[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Food and beverages in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "food_and_bev = pd.read_csv('data/cpi_data/food_and_bev.csv')\n",
    "food_and_bev = food_and_bev.drop(columns=['Series ID'])\n",
    "food_and_bev = food_and_bev.drop(columns=['Period'])\n",
    "food_and_bev['Month'] = food_and_bev['Label'].apply(lambda x: x.split()[1])\n",
    "food_and_bev = food_and_bev.drop(columns=['Label'])\n",
    "food_and_bev = food_and_bev[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Food in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "food = pd.read_csv('data/cpi_data/just_food.csv')\n",
    "food = food.drop(columns=['Series ID'])\n",
    "food = food.drop(columns=['Period'])\n",
    "food['Month'] = food['Label'].apply(lambda x: x.split()[1])\n",
    "food = food.drop(columns=['Label'])\n",
    "food = food[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Prescription drugs in U.S. city average, all urban consumers, not seasonally adjusted\n",
    "presc_drugs = pd.read_csv('data/cpi_data/presc_drugs.csv')\n",
    "presc_drugs = presc_drugs.drop(columns=['Series ID'])\n",
    "presc_drugs = presc_drugs.drop(columns=['Period'])\n",
    "presc_drugs['Month'] = presc_drugs['Label'].apply(lambda x: x.split()[1])\n",
    "presc_drugs = presc_drugs.drop(columns=['Label'])\n",
    "presc_drugs = presc_drugs[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Commodities in U.S. city average, all urban consumers, seasonally adjusted\n",
    "commodities = pd.read_csv('data/cpi_data/commodities.csv')\n",
    "commodities = commodities.drop(columns=['Series ID'])\n",
    "commodities = commodities.drop(columns=['Period'])\n",
    "commodities['Month'] = commodities['Label'].apply(lambda x: x.split()[1])\n",
    "commodities = commodities.drop(columns=['Label'])\n",
    "commodities = commodities[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Durables in U.S. city average, all urban consumers, seasonally adjusted\n",
    "durables = pd.read_csv('data/cpi_data/durables.csv')\n",
    "durables = durables.drop(columns=['Series ID'])\n",
    "durables = durables.drop(columns=['Period'])\n",
    "durables['Month'] = durables['Label'].apply(lambda x: x.split()[1])\n",
    "durables = durables.drop(columns=['Label'])\n",
    "durables = durables[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Nondurables in U.S. city average, all urban consumers, seasonally adjusted\n",
    "nondurables = pd.read_csv('data/cpi_data/nondurables.csv')\n",
    "nondurables = nondurables.drop(columns=['Series ID'])\n",
    "nondurables = nondurables.drop(columns=['Period'])\n",
    "nondurables['Month'] = nondurables['Label'].apply(lambda x: x.split()[1])\n",
    "nondurables = nondurables.drop(columns=['Label'])\n",
    "nondurables = nondurables[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Recreation in U.S. city average, all urban consumers, seasonally adjusted\n",
    "rec = pd.read_csv('data/cpi_data/recreation.csv')\n",
    "rec = rec.drop(columns=['Series ID'])\n",
    "rec = rec.drop(columns=['Period'])\n",
    "rec['Month'] = rec['Label'].apply(lambda x: x.split()[1])\n",
    "rec = rec.drop(columns=['Label'])\n",
    "rec = rec[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Appliances in U.S. city average, all urban consumers, seasonally adjusted\n",
    "appliances = pd.read_csv('data/cpi_data/appliances.csv')\n",
    "appliances = appliances.drop(columns=['Series ID'])\n",
    "appliances = appliances.drop(columns=['Period'])\n",
    "appliances['Month'] = appliances['Label'].apply(lambda x: x.split()[1])\n",
    "appliances = appliances.drop(columns=['Label'])\n",
    "appliances = appliances[['Year', 'Month', 'Value']]\n",
    "\n",
    "\n",
    "# Toys in U.S. city average, all urban consumers, seasonally adjusted\n",
    "toys = pd.read_csv('data/cpi_data/toys.csv')\n",
    "toys = toys.drop(columns=['Series ID'])\n",
    "toys = toys.drop(columns=['Period'])\n",
    "toys['Month'] = toys['Label'].apply(lambda x: x.split()[1])\n",
    "toys = toys.drop(columns=['Label'])\n",
    "toys = toys[['Year', 'Month', 'Value']]\n",
    "\n",
    "\n",
    "# Apparel in U.S. city average, all urban consumers, seasonally adjusted\n",
    "apparel = pd.read_csv('data/cpi_data/apparel.csv')\n",
    "apparel = apparel.drop(columns=['Series ID'])\n",
    "apparel = apparel.drop(columns=['Period'])\n",
    "apparel['Month'] = apparel['Label'].apply(lambda x: x.split()[1])\n",
    "apparel = apparel.drop(columns=['Label'])\n",
    "apparel = apparel[['Year', 'Month', 'Value']]\n",
    "\n",
    "# Combine dataframes\n",
    "all_cpi = pd.DataFrame(columns = ['Year', 'Month', 'all_items_NSA', 'all_items_SA', 'food_and_bev', 'food', 'presc_drugs', 'commodities', 'durables', 'nondurables', 'recreation', 'appliances', 'toys', 'apparel'])\n",
    "all_cpi['Year'] = all_items_NSA['Year']\n",
    "all_cpi['Month'] = all_items_NSA['Month']\n",
    "all_cpi['all_items_NSA'] = all_items_NSA['Value']\n",
    "all_cpi['all_items_SA'] = all_items_SA['Value']\n",
    "all_cpi['food_and_bev'] = food_and_bev['Value']\n",
    "all_cpi['food'] = food['Value']\n",
    "all_cpi['presc_drugs'] = presc_drugs['Value']\n",
    "all_cpi['commodities'] = commodities['Value']\n",
    "all_cpi['durables'] = durables['Value']\n",
    "all_cpi['nondurables'] = nondurables['Value']\n",
    "all_cpi['recreation'] = rec['Value']\n",
    "all_cpi['appliances'] = appliances['Value']\n",
    "all_cpi['toys'] = toys['Value']\n",
    "all_cpi['apparel'] = apparel['Value']\n",
    "\n",
    "all_cpi.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melt the category columns to get the data into tidy format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cpi = all_cpi.melt(\n",
    "    id_vars=['Year', 'Month'],\n",
    "    var_name='Category',\n",
    "    value_name='Value'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>Jan</td>\n",
       "      <td>all_items_NSA</td>\n",
       "      <td>261.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>Feb</td>\n",
       "      <td>all_items_NSA</td>\n",
       "      <td>263.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>Mar</td>\n",
       "      <td>all_items_NSA</td>\n",
       "      <td>264.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>Apr</td>\n",
       "      <td>all_items_NSA</td>\n",
       "      <td>267.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>May</td>\n",
       "      <td>all_items_NSA</td>\n",
       "      <td>269.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>2024</td>\n",
       "      <td>Sep</td>\n",
       "      <td>apparel</td>\n",
       "      <td>132.886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>2024</td>\n",
       "      <td>Oct</td>\n",
       "      <td>apparel</td>\n",
       "      <td>131.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>2024</td>\n",
       "      <td>Nov</td>\n",
       "      <td>apparel</td>\n",
       "      <td>131.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>2024</td>\n",
       "      <td>Dec</td>\n",
       "      <td>apparel</td>\n",
       "      <td>131.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>2025</td>\n",
       "      <td>Jan</td>\n",
       "      <td>apparel</td>\n",
       "      <td>130.160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>588 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year Month       Category    Value\n",
       "0    2021   Jan  all_items_NSA  261.582\n",
       "1    2021   Feb  all_items_NSA  263.014\n",
       "2    2021   Mar  all_items_NSA  264.877\n",
       "3    2021   Apr  all_items_NSA  267.054\n",
       "4    2021   May  all_items_NSA  269.195\n",
       "..    ...   ...            ...      ...\n",
       "583  2024   Sep        apparel  132.886\n",
       "584  2024   Oct        apparel  131.706\n",
       "585  2024   Nov        apparel  131.814\n",
       "586  2024   Dec        apparel  131.986\n",
       "587  2025   Jan        apparel  130.160\n",
       "\n",
       "[588 rows x 4 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thoughtful discussion of ethical concerns included\n",
    "- Ethical concerns consider the whole data science process (question asked, data collected, data being used, the bias in data, analysis, post-analysis, etc.)\n",
    "- How your group handled bias/ethical concerns clearly described\n",
    "    - Discussed the main points of Deon’s Ethics Checklist in a meeting together. Reviewed each section as a team before submission. \n",
    "\n",
    "Acknowledge and address any ethics & privacy related issues of your question(s), proposed dataset(s), and/or analyses. Use the information provided in lecture to guide your group discussion and thinking. If you need further guidance, check out [Deon's Ethics Checklist](http://deon.drivendata.org/#data-science-ethics-checklist). In particular:\n",
    "\n",
    "- Are there any biases/privacy/terms of use issues with the data you propsed?\n",
    "    - If we decide to use an API to collect our Amazon pricing data such as Keepa, we intend to comply with the Keepa data security policies and API terms of service. \n",
    "- Are there potential biases in your dataset(s), in terms of who it composes, and how it was collected, that may be problematic in terms of it allowing for equitable analysis? (For example, does your data exclude particular populations, or is it likely to reflect particular human biases in a way that could be a problem?)\n",
    "    - If we decide to use an API to collect our Amazon pricing data such as Keepa, we intend to comply with the Keepa data security policies and API terms of service. \n",
    "- How will you set out to detect these specific biases before, during, and after/when communicating your analysis?\n",
    "    - We will start in the data collection process, and ensure that we are collecting data in an unbiased way to effectively represent the Amazon markets while also focusing on aligning these samples with CPI sectors fairly. After we have collected the data, we will conduct EDA to ensure that we did not miss any potential biases in the data collection process. \n",
    "- Are there any other issues related to your topic area, data, and/or analyses that are potentially problematic in terms of data privacy and equitable impact?\n",
    "    - We want to make it clear that regardless of our findings, we are only establishing a correlation between Amazon product prices and CPI rates, and not a causal relationship. Any results are not conclusive evidence that Amazon is responsible for any part of inflation in the US. If our analysis is misinterpreted, it could lead to false narratives about Amazon’s role in inflation. We intend to frame conclusions carefully and provide balanced interpretations of our results. \n",
    "- Personally Identifiable Information\n",
    "    - No personally identifiable information will be collected. However, some sellers may be identifiable, especially if analyzing price trends for specific brands or niche categories. Seller information will be removed and/or hidden from our analysis.\n",
    "- Metric Selection:\n",
    "    - Use percentage price change instead of raw price differences, ensuring fair comparisons across categories.\n",
    "- Explainability:\n",
    "    - Clearly explain findings to avoid speculative claims about Amazon's role in inflation, labor costs, price matching, etc.\n",
    "- Communication Limitation:\n",
    "    - Highlight that CPI and Amazon prices are not perfect comparisons, and results should be interpreted with caution.\n",
    "- Transparency: \n",
    "    - Since access to Keepa data requires a paid membership, it reduces transparency and reproducibility for others who may want to replicate the study. To mitigate this, we minted to provide detailed documentation on how the data was obtained and which filters were applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* *Meet twice a week, Friday at 5pm and Wednesdays around 8:30pm.*\n",
    "\n",
    "* *Delegate work as we see fit in order to meet deadlines, no one group member has one specific role, all types of work will be shared.*\n",
    "\n",
    "* *Openness to ideas. Be respectful and kind to each other if there is disagreement.*\n",
    "\n",
    "* *Meeting primarily on zoom, and keeping consistent communication when not meeting via a group chat (iMessage)*\n",
    "\n",
    "* *Allow space for input and opinions from all members, and come to decisions about our course of action unanimously. Discuss conflicts and ideas until each member feels comfortable with the final decision.*\n",
    "\n",
    "* *If there is an pressing outside factor (personal life, exams, etc.) communicate with each other and find another course of action, work can be delegated differently if need be.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/07  |  5 PM | Brainstorm Topics / Review Project Proposal  | Finalize a topic and start searching for datasets | \n",
    "| 2/09  |  8:30 PM |  N/A | Discuss amongst eachother goals for the project and review/submit research proposal | \n",
    "| 2/12  | 8:30 PM  | Search for any more possible Datasets  | Discuss how to divvy up our project and make a broad outline   |\n",
    "| 2/14  | 5 PM  | Import our Datasets and start wrangling | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/21  | 5 PM  | Finalize wrangling/EDA; Begin Analysis | Discuss/edit Analysis; Finish Ceck in #1 |\n",
    "| 3/7  |  5 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project ; Check in #2 - Complete EDA |\n",
    "| 3/14  | 5 PM  | Practice Video Presentation | Record our Project Presentation |\n",
    "| 3/19  | 5 PM  | Go over each section of our Project | Finialize Files and Submit |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
